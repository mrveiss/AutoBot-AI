#!/usr/bin/env python3
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""
Comprehensive Code Quality Analysis Dashboard
Runs all analyzers and provides unified quality metrics and recommendations

NOTE: run_comprehensive_quality_analysis (~145 lines) is an ACCEPTABLE EXCEPTION
per Issue #490 - analysis dashboard with sequential report generation. Low priority.
"""

import asyncio
import json
from pathlib import Path

from src.code_quality_dashboard import CodeQualityDashboard


def _print_executive_metrics(metrics: dict, issues: dict, report: dict) -> None:
    """
    Print executive-level quality metrics summary.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print(f"ğŸ“Š **Overall Quality Assessment:**")
    print(f"   ğŸ¯ Overall Quality Score: {metrics['overall_score']}/100")
    print(f"   ğŸ“‹ Total Issues Found: {issues['total_issues']}")
    print(f"   ğŸš¨ Critical Issues: {issues['critical_issues']}")
    print(f"   âš ï¸  High Priority Issues: {issues['high_priority_issues']}")
    print(f"   ğŸ“ Files Analyzed: {report['files_analyzed']}")
    print(f"   â±ï¸  Analysis Time: {report['analysis_time_seconds']:.2f} seconds")
    print()

    # Category breakdown
    print("ğŸ·ï¸  **Issues by Category:**")
    for category, count in issues['by_category'].items():
        category_name = category.replace('_', ' ').title()
        print(f"   â€¢ {category_name}: {count} issues")
    print()


def _print_analyzer_scores(metrics: dict) -> None:
    """
    Print individual analyzer scores with visual indicators.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸ” **Individual Analysis Scores:**")
    score_categories = [
        ('Security', metrics['security_score'], 'ğŸ›¡ï¸'),
        ('Performance', metrics['performance_score'], 'âš¡'),
        ('Architecture', metrics['architecture_score'], 'ğŸ—ï¸'),
        ('Test Coverage', metrics['test_coverage_score'], 'ğŸ§ª'),
        ('API Consistency', metrics['api_consistency_score'], 'ğŸ”—'),
        ('Code Duplication', metrics['code_duplication_score'], 'â™»ï¸'),
        ('Environment Config', metrics['environment_config_score'], 'âš™ï¸'),
    ]

    for name, score, emoji in score_categories:
        status = get_score_status(score)
        status_color = get_status_emoji(score)
        print(f"   {emoji} {name}: {score}/100 {status_color} {status}")
    print()


def _print_technical_debt(debt: dict) -> None:
    """
    Print technical debt analysis and effort breakdown.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸ’¸ **Technical Debt Analysis:**")
    print(f"   ğŸ“Š Total Estimated Effort: {debt['estimated_total_effort_days']} days ({debt['estimated_total_effort_hours']} hours)")
    print(f"   ğŸš¨ Critical Issues Effort: {debt['estimated_critical_effort_hours']} hours")
    print(f"   ğŸ“ˆ Debt Ratio: {debt['debt_ratio']}% of total project")
    print()

    print("ğŸ’° **Effort by Category:**")
    for category, data in debt['effort_by_category'].items():
        category_name = category.replace('_', ' ').title()
        print(f"   â€¢ {category_name}: {data['count']} issues, {data['effort_hours']} hours")
    print()


def _print_priority_issues(report: dict) -> None:
    """
    Print top priority issues requiring immediate action.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸš¨ **Top Priority Issues (Immediate Action Required):**")
    critical_issues = [issue for issue in report['prioritized_issues'] if issue['severity'] == 'critical']
    high_issues = [issue for issue in report['prioritized_issues'] if issue['severity'] == 'high']
    top_issues = critical_issues[:5] + high_issues[:5]

    for i, issue in enumerate(top_issues[:10], 1):
        severity_emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
        emoji = severity_emoji.get(issue['severity'], "âšª")

        print(f"\n{i}. {emoji} **{issue['title']}** ({issue['severity'].upper()})")
        print(f"   ğŸ“‚ Category: {issue['category'].replace('_', ' ').title()}")
        if issue['file_path'] != 'Multiple files':
            print(f"   ğŸ“„ File: {issue['file_path']}:{issue['line_number']}")
        print(f"   ğŸ“ Description: {issue['description']}")
        print(f"   ğŸ’¡ Fix: {issue['fix_suggestion']}")
        print(f"   ğŸ”§ Effort: {issue['estimated_effort'].title()}")
        print(f"   ğŸ¯ Priority Score: {issue['priority_score']}/100")
    print()


def _print_analysis_alerts(report: dict) -> None:
    """
    Print critical alerts for security, performance, and testing.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    # Security-specific analysis
    if report['detailed_analyses'].get('security'):
        security_data = report['detailed_analyses']['security']
        if security_data.get('critical_vulnerabilities', 0) > 0:
            print("ğŸ›¡ï¸ **CRITICAL SECURITY ALERT:**")
            print(f"   Found {security_data['critical_vulnerabilities']} critical security vulnerabilities!")
            print("   These must be addressed immediately before deployment.")
            print()

    # Performance-specific analysis
    if report['detailed_analyses'].get('performance'):
        perf_data = report['detailed_analyses']['performance']
        if perf_data.get('critical_issues', 0) > 0:
            print("âš¡ **CRITICAL PERFORMANCE ALERT:**")
            print(f"   Found {perf_data['critical_issues']} critical performance issues!")
            print("   These may cause memory leaks or system instability.")
            print()

    # Testing coverage analysis
    if report['detailed_analyses'].get('testing_coverage'):
        test_data = report['detailed_analyses']['testing_coverage']
        coverage = test_data.get('test_coverage_percentage', 0)
        print(f"ğŸ§ª **Testing Coverage Analysis:**")
        print(f"   Current test coverage: {coverage}%")
        if coverage < 70:
            print("   âš ï¸  Coverage is below recommended 70% threshold")
            print("   Consider adding more unit and integration tests")
        print()


async def run_comprehensive_quality_analysis():
    """Run comprehensive code quality analysis"""

    print("ğŸ¯ Starting comprehensive code quality analysis...")
    print("This will run all available analyzers:")
    print("  â€¢ Code Duplication Analyzer")
    print("  â€¢ Environment Variable Analyzer")
    print("  â€¢ Performance & Memory Leak Analyzer")
    print("  â€¢ Security Vulnerability Analyzer")
    print("  â€¢ API Consistency Analyzer")
    print("  â€¢ Testing Coverage Gap Analyzer")
    print("  â€¢ Architectural Pattern Analyzer")
    print()

    dashboard = CodeQualityDashboard()

    # Generate comprehensive report
    report = await dashboard.generate_comprehensive_report(
        root_path=".",
        patterns=["src/**/*.py", "backend/**/*.py"],
        include_trends=True
    )

    print("=== Code Quality Executive Summary ===\n")

    # Executive summary
    summary = await dashboard.generate_executive_summary(report)
    print(summary)

    print("\n=== Detailed Quality Analysis Results ===\n")

    # Issue #281: Use extracted helpers for dashboard sections
    metrics = report['quality_metrics']
    issues = report['issue_summary']
    debt = report['technical_debt']

    _print_executive_metrics(metrics, issues, report)
    _print_analyzer_scores(metrics)
    _print_technical_debt(debt)
    _print_priority_issues(report)
    _print_analysis_alerts(report)

    # Improvement recommendations
    print("ğŸ“‹ **Improvement Recommendations (Priority Order):**")
    for i, recommendation in enumerate(report['improvement_recommendations'], 1):
        print(f"{i}. {recommendation}")
    print()

    # Quality trends (if available)
    if report.get('quality_trends'):
        print("ğŸ“ˆ **Quality Trends:**")
        trends = report['quality_trends']
        if len(trends) > 1:
            latest = trends[0]
            previous = trends[1]
            score_change = latest['overall_score'] - previous['overall_score']
            trend_emoji = "ğŸ“ˆ" if score_change > 0 else "ğŸ“‰" if score_change < 0 else "â¡ï¸"
            print(f"   {trend_emoji} Score change: {score_change:+.1f} points")

            issue_change = latest['issue_count'] - previous['issue_count']
            issue_emoji = "ğŸ“‰" if issue_change < 0 else "ğŸ“ˆ" if issue_change > 0 else "â¡ï¸"
            print(f"   {issue_emoji} Issue count change: {issue_change:+d}")
        else:
            print("   ğŸ“Š Baseline measurement established")
        print()

    # Detailed analysis summaries
    print("ğŸ“Š **Detailed Analysis Summaries:**")

    # Issue #315: Refactored with data-driven formatting instead of elif chain
    analysis_formats = {
        'duplication': {
            'name': 'Code Duplication', 'emoji': 'â™»ï¸',
            'fields': [
                ('total_duplicate_groups', 'Found {} duplicate code groups'),
                ('total_lines_saved', 'Potential lines saved: {}'),
            ]
        },
        'environment': {
            'name': 'Environment Variables', 'emoji': 'âš™ï¸',
            'fields': [
                ('total_hardcoded_values', 'Found {} hardcoded values'),
                ('critical_hardcoded_values', 'Critical values: {}'),
            ]
        },
        'security': {
            'name': 'Security', 'emoji': 'ğŸ›¡ï¸',
            'fields': [
                ('total_vulnerabilities', 'Found {} potential vulnerabilities'),
                ('critical_vulnerabilities', 'Critical vulnerabilities: {}'),
            ]
        },
        'performance': {
            'name': 'Performance', 'emoji': 'âš¡',
            'fields': [
                ('total_performance_issues', 'Found {} performance issues'),
                ('critical_issues', 'Critical issues: {}'),
            ]
        },
        'api_consistency': {
            'name': 'API Consistency', 'emoji': 'ğŸ”—',
            'fields': [
                ('total_endpoints', 'Analyzed {} API endpoints'),
                ('inconsistencies_found', 'Found {} consistency issues'),
            ]
        },
        'testing_coverage': {
            'name': 'Testing Coverage', 'emoji': 'ğŸ§ª',
            'fields': [
                ('total_functions', 'Analyzed {} functions'),
                ('test_coverage_percentage', 'Test coverage: {}%'),
            ]
        },
        'architecture': {
            'name': 'Architecture', 'emoji': 'ğŸ—ï¸',
            'fields': [
                ('total_components', 'Analyzed {} architectural components'),
                ('architectural_issues', 'Found {} architectural issues'),
            ]
        },
    }

    for analysis_type, fmt in analysis_formats.items():
        data = report['detailed_analyses'].get(analysis_type)
        if data:
            print(f"\n{fmt['emoji']} **{fmt['name']} Analysis:**")
            for field_key, field_fmt in fmt['fields']:
                value = data.get(field_key, 0)
                print(f"   â€¢ {field_fmt.format(value)}")

    # Save comprehensive report
    report_path = Path("comprehensive_quality_report.json")
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2, default=str)

    # Generate summary report
    summary_path = Path("quality_summary.txt")
    with open(summary_path, 'w') as f:
        f.write(summary)

    print(f"\n=== Reports Generated ===")
    print(f"ğŸ“‹ Comprehensive report: {report_path}")
    print(f"ğŸ“„ Executive summary: {summary_path}")

    return report


def get_score_status(score: float) -> str:
    """Get human-readable status for score"""
    if score >= 90:
        return "EXCELLENT"
    elif score >= 80:
        return "GOOD"
    elif score >= 70:
        return "FAIR"
    elif score >= 60:
        return "NEEDS IMPROVEMENT"
    else:
        return "CRITICAL"


def get_status_emoji(score: float) -> str:
    """Get status emoji based on score"""
    if score >= 90:
        return "ğŸŸ¢"
    elif score >= 80:
        return "ğŸŸ¡"
    elif score >= 70:
        return "ğŸŸ "
    else:
        return "ğŸ”´"


async def generate_action_plan(report):
    """Generate specific action plan based on results"""

    print("\n=== ğŸ“‹ Recommended Action Plan ===")

    metrics = report['quality_metrics']
    issues = report['issue_summary']

    # Phase 1: Critical Issues (Week 1)
    critical_count = issues['critical_issues']
    if critical_count > 0:
        print(f"\nğŸš¨ **Phase 1: Critical Issues (IMMEDIATE - Week 1)**")
        print(f"   Address {critical_count} critical issues:")

        critical_issues = [i for i in report['prioritized_issues'] if i['severity'] == 'critical']
        for issue in critical_issues[:5]:  # Top 5 critical
            print(f"   â€¢ {issue['title']}")
            print(f"     Action: {issue['fix_suggestion']}")

    # Phase 2: High Priority (Weeks 2-3)
    high_count = issues['high_priority_issues']
    if high_count > 0:
        print(f"\nâš ï¸  **Phase 2: High Priority (Weeks 2-3)**")
        print(f"   Address {high_count} high priority issues:")

        if metrics['security_score'] < 80:
            print("   â€¢ Complete security vulnerability audit")
        if metrics['performance_score'] < 70:
            print("   â€¢ Fix performance bottlenecks and memory leaks")
        if metrics['test_coverage_score'] < 70:
            print("   â€¢ Increase test coverage to 80%+")

    # Phase 3: Quality Improvements (Month 2)
    print(f"\nğŸ”§ **Phase 3: Quality Improvements (Month 2)**")
    if metrics['architecture_score'] < 80:
        print("   â€¢ Refactor architectural issues")
    if metrics['code_duplication_score'] < 80:
        print("   â€¢ Eliminate code duplication")
    if metrics['api_consistency_score'] < 80:
        print("   â€¢ Standardize API patterns")

    # Phase 4: Maintenance & Monitoring (Ongoing)
    print(f"\nğŸ“ˆ **Phase 4: Continuous Improvement (Ongoing)**")
    print("   â€¢ Set up automated quality monitoring")
    print("   â€¢ Implement pre-commit quality checks")
    print("   â€¢ Regular quality reviews (weekly)")
    print("   â€¢ Update team coding standards")

    # Estimated timeline
    debt = report['technical_debt']
    total_days = debt['estimated_total_effort_days']
    critical_hours = debt['estimated_critical_effort_hours']

    print(f"\nâ° **Estimated Timeline:**")
    print(f"   â€¢ Critical fixes: {critical_hours} hours (1-2 weeks)")
    print(f"   â€¢ Total remediation: {total_days} days ({total_days/5:.1f} weeks)")
    print(f"   â€¢ Team of 2-3 developers recommended")


async def main():
    """Run comprehensive code quality analysis"""

    # Run analysis
    report = await run_comprehensive_quality_analysis()

    # Generate action plan
    await generate_action_plan(report)

    print("\n=== ğŸ¯ Analysis Complete ===")
    print("Next Steps:")
    print("1. Review comprehensive_quality_report.json for detailed findings")
    print("2. Start with critical security and performance issues")
    print("3. Follow the recommended action plan phases")
    print("4. Set up automated quality monitoring")
    print("5. Schedule regular quality reviews")


if __name__ == "__main__":
    asyncio.run(main())
