#!/bin/bash
# Setup Model Sharing Between Windows and WSL Containers
# This script helps configure different model sharing scenarios

set -e

echo "ðŸš€ AutoBot Ollama Model Sharing Setup"
echo "======================================"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Detect current environment
detect_environment() {
    print_status "Detecting environment..."

    if [[ -n "$WSL_DISTRO_NAME" ]]; then
        echo "Environment: WSL2 ($WSL_DISTRO_NAME)"
        IS_WSL=true
    elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
        echo "Environment: Native Linux"
        IS_WSL=false
    else
        print_error "Unsupported environment: $OSTYPE"
        exit 1
    fi
}

# Check if Ollama is running on Windows
check_windows_ollama() {
    print_status "Checking for Windows Ollama..."

    if curl -s "http://127.0.0.2:11434/api/tags" >/dev/null 2>&1; then
        print_success "Windows Ollama detected at 127.0.0.2:11434"
        HAS_WINDOWS_OLLAMA=true

        # Get model list
        WINDOWS_MODELS=$(curl -s "http://127.0.0.2:11434/api/tags" | jq -r '.models[].name' 2>/dev/null || echo "")
        if [[ -n "$WINDOWS_MODELS" ]]; then
            print_success "Found models on Windows Ollama:"
            echo "$WINDOWS_MODELS" | sed 's/^/  - /'
        fi
    else
        print_warning "Windows Ollama not detected"
        HAS_WINDOWS_OLLAMA=false
    fi
}

# Check if Ollama is running in WSL
check_wsl_ollama() {
    print_status "Checking for WSL Ollama..."

    if command -v ollama >/dev/null 2>&1; then
        print_success "Ollama command found in WSL"

        if curl -s "http://127.0.0.1:11434/api/tags" >/dev/null 2>&1; then
            print_success "WSL Ollama service running"
            HAS_WSL_OLLAMA=true

            # Get model list
            WSL_MODELS=$(curl -s "http://127.0.0.1:11434/api/tags" | jq -r '.models[].name' 2>/dev/null || echo "")
            if [[ -n "$WSL_MODELS" ]]; then
                print_success "Found models in WSL Ollama:"
                echo "$WSL_MODELS" | sed 's/^/  - /'
            fi
        else
            print_warning "WSL Ollama installed but not running"
            HAS_WSL_OLLAMA=false
        fi
    else
        print_warning "Ollama not installed in WSL"
        HAS_WSL_OLLAMA=false
    fi
}

# Find Ollama model directories
find_model_directories() {
    print_status "Finding Ollama model directories..."

    # Windows Ollama models (via WSL mount)
    if [[ $IS_WSL == true ]]; then
        WINDOWS_USER=$(powershell.exe -Command "echo \$env:USERNAME" 2>/dev/null | tr -d '\r' || echo "")
        if [[ -n "$WINDOWS_USER" ]]; then
            WIN_OLLAMA_DIR="/mnt/c/Users/$WINDOWS_USER/.ollama"
            if [[ -d "$WIN_OLLAMA_DIR" ]]; then
                print_success "Found Windows Ollama directory: $WIN_OLLAMA_DIR"
                WINDOWS_OLLAMA_PATH="$WIN_OLLAMA_DIR"
            fi
        fi
    fi

    # WSL Ollama models
    WSL_OLLAMA_DIR="$HOME/.ollama"
    if [[ -d "$WSL_OLLAMA_DIR" ]]; then
        print_success "Found WSL Ollama directory: $WSL_OLLAMA_DIR"
        WSL_OLLAMA_PATH="$WSL_OLLAMA_DIR"
    fi
}

# Setup model sharing configuration
setup_model_sharing() {
    print_status "Setting up model sharing configuration..."

    # Create environment file for model sharing
    ENV_FILE=".env.model-sharing"

    echo "# Ollama Model Sharing Configuration" > "$ENV_FILE"
    echo "# Generated by setup_model_sharing.sh" >> "$ENV_FILE"
    echo "" >> "$ENV_FILE"

    if [[ -n "$WINDOWS_USER" ]]; then
        echo "WINDOWS_USERNAME=$WINDOWS_USER" >> "$ENV_FILE"
    fi

    if [[ -n "$WINDOWS_OLLAMA_PATH" ]]; then
        echo "WINDOWS_OLLAMA_PATH=$WINDOWS_OLLAMA_PATH" >> "$ENV_FILE"
    fi

    if [[ -n "$WSL_OLLAMA_PATH" ]]; then
        echo "WSL_OLLAMA_PATH=$WSL_OLLAMA_PATH" >> "$ENV_FILE"
    fi

    # Determine best strategy
    if [[ $HAS_WINDOWS_OLLAMA == true ]] && [[ -n "$WINDOWS_OLLAMA_PATH" ]]; then
        echo "# Use Windows Ollama models in containers" >> "$ENV_FILE"
        echo "AUTOBOT_OLLAMA_HOST=127.0.0.2" >> "$ENV_FILE"
        echo "AUTOBOT_OLLAMA_MODELS_PATH=$WINDOWS_OLLAMA_PATH" >> "$ENV_FILE"
        echo "AUTOBOT_MODEL_SHARING_STRATEGY=windows" >> "$ENV_FILE"
        STRATEGY="windows"
    elif [[ $HAS_WSL_OLLAMA == true ]] && [[ -n "$WSL_OLLAMA_PATH" ]]; then
        echo "# Use WSL Ollama models in containers" >> "$ENV_FILE"
        echo "AUTOBOT_OLLAMA_HOST=127.0.0.1" >> "$ENV_FILE"
        echo "AUTOBOT_OLLAMA_MODELS_PATH=$WSL_OLLAMA_PATH" >> "$ENV_FILE"
        echo "AUTOBOT_MODEL_SHARING_STRATEGY=wsl" >> "$ENV_FILE"
        STRATEGY="wsl"
    else
        echo "# No existing Ollama found - use container-only" >> "$ENV_FILE"
        echo "AUTOBOT_MODEL_SHARING_STRATEGY=container" >> "$ENV_FILE"  # pragma: allowlist secret
        STRATEGY="container"
    fi

    print_success "Created configuration: $ENV_FILE"
    print_status "Recommended strategy: $STRATEGY"
}

# Show recommendations
show_recommendations() {
    echo ""
    echo "ðŸŽ¯ Recommendations:"
    echo "=================="

    case $STRATEGY in
        "windows")
            echo "âœ… Use Windows Ollama with model sharing:"
            echo "   - Models stored on Windows, accessible to containers"
            echo "   - Run: docker-compose -f docker/compose/docker-compose.shared-models.yml up"
            echo "   - Set: source .env.model-sharing"
            ;;
        "wsl")
            echo "âœ… Use WSL Ollama with model sharing:"
            echo "   - Models stored in WSL, accessible to containers"
            echo "   - Run: docker-compose -f docker/compose/docker-compose.shared-models.yml up"
            echo "   - Set: source .env.model-sharing"
            ;;
        "container")
            echo "âœ… Use container-only Ollama:"
            echo "   - Models stored in Docker volume"
            echo "   - Run: docker-compose -f docker/compose/docker-compose.full.yml up"
            echo "   - Download models inside container"
            ;;
    esac

    echo ""
    echo "ðŸ”§ Model Management Commands:"
    echo "============================"

    if [[ $STRATEGY == "windows" ]] || [[ $STRATEGY == "wsl" ]]; then
        echo "# List available models"
        echo "curl http://127.0.0.2:11434/api/tags"
        echo ""
        echo "# Pull new model (to host)"
        echo "ollama pull deepseek-r1:14b"
        echo ""
        echo "# Models will be automatically available to containers"
    else
        echo "# Access container Ollama"
        echo "docker exec -it autobot-ollama-shared ollama list"
        echo ""
        echo "# Pull model in container"
        echo "docker exec -it autobot-ollama-shared ollama pull deepseek-r1:14b"
    fi
}

# Main execution
main() {
    detect_environment
    check_windows_ollama
    check_wsl_ollama
    find_model_directories
    setup_model_sharing
    show_recommendations

    print_success "Model sharing setup complete!"
    print_status "Review the generated .env.model-sharing file and follow the recommendations above."
}

# Run main function
main
