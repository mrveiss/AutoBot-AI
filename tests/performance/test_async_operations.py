"""
Performance Load Testing for Async Operations - Task 2.5

Validates that async operations deliver 10-50x performance improvement under concurrent load.

Test Scenarios:
1. 50 Concurrent Chat Requests (HTTP Load via Locust)
2. 100 Concurrent Redis Operations (Async Operations)
3. Mixed File I/O and Redis Operations (Combined Load)
4. Cross-VM Concurrent Requests (Distributed Testing)

Success Criteria:
- 10-50x improvement in response time
- p95 latency < 2 seconds
- No event loop blocking detected
- 50+ concurrent users supported
- Zero timeouts or errors under load

Author: AutoBot Performance Team
Date: 2025-10-06
"""

import asyncio
import json
import logging
import statistics
import time
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor

import httpx
import numpy as np
import pytest
from locust import HttpUser, task, between, events
from locust.env import Environment
from locust.stats import stats_printer, stats_history

# Add project root to path
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.utils.async_redis_manager import get_redis_manager, AsyncRedisManager
from src.chat_workflow_manager import ChatWorkflowManager

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@dataclass
class PerformanceMetrics:
    """Performance metrics for test scenarios"""
    test_name: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p50_latency: float
    p95_latency: float
    p99_latency: float
    requests_per_second: float
    event_loop_blocked: bool = False
    error_rate: float = 0.0
    response_times: List[float] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary for JSON export"""
        return {
            "test_name": self.test_name,
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.failed_requests,
            "total_time": self.total_time,
            "avg_response_time": self.avg_response_time,
            "min_response_time": self.min_response_time,
            "max_response_time": self.max_response_time,
            "p50_latency": self.p50_latency,
            "p95_latency": self.p95_latency,
            "p99_latency": self.p99_latency,
            "requests_per_second": self.requests_per_second,
            "event_loop_blocked": self.event_loop_blocked,
            "error_rate": self.error_rate
        }


class PerformanceReportGenerator:
    """Generate performance test reports in JSON and Markdown formats"""

    def __init__(self, output_dir: str = "reports/performance"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def generate_report(self, metrics: List[PerformanceMetrics], baseline: Optional[Dict] = None):
        """Generate comprehensive performance report"""
        # Generate JSON report
        json_report = self._generate_json_report(metrics)
        json_path = self.output_dir / f"async_load_test_results_{self.timestamp}.json"

        with open(json_path, 'w') as f:
            json.dump(json_report, f, indent=2)

        logger.info(f"‚úÖ JSON report saved to {json_path}")

        # Generate Markdown report
        md_report = self._generate_markdown_report(metrics, baseline)
        md_path = self.output_dir / f"async_load_test_report_{self.timestamp}.md"

        with open(md_path, 'w') as f:
            f.write(md_report)

        logger.info(f"‚úÖ Markdown report saved to {md_path}")

        return {
            "json_path": str(json_path),
            "markdown_path": str(md_path)
        }

    def _generate_json_report(self, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
        """Generate JSON performance report"""
        return {
            "timestamp": datetime.now().isoformat(),
            "test_suite": "Async Operations Load Testing",
            "version": "1.0",
            "metrics": [m.to_dict() for m in metrics],
            "summary": {
                "total_tests": len(metrics),
                "all_passed": all(not m.event_loop_blocked and m.error_rate < 0.01 for m in metrics),
                "avg_p95_latency": statistics.mean([m.p95_latency for m in metrics]),
                "total_requests": sum(m.total_requests for m in metrics)
            }
        }

    def _generate_markdown_report(self, metrics: List[PerformanceMetrics], baseline: Optional[Dict] = None) -> str:
        """Generate Markdown performance report"""
        report = f"""# Async Operations Load Testing Report

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Executive Summary

This report validates that async operations deliver 10-50x performance improvement under concurrent load.

### Test Suite Results

| Test Scenario | Requests | Success Rate | P95 Latency | Event Loop |
|--------------|----------|--------------|-------------|------------|
"""

        for m in metrics:
            success_rate = (m.successful_requests / m.total_requests * 100) if m.total_requests > 0 else 0
            event_loop_status = "‚ùå BLOCKED" if m.event_loop_blocked else "‚úÖ OK"
            report += f"| {m.test_name} | {m.total_requests} | {success_rate:.1f}% | {m.p95_latency*1000:.2f}ms | {event_loop_status} |\n"

        report += "\n## Detailed Metrics\n\n"

        for m in metrics:
            report += f"""### {m.test_name}

**Performance Metrics:**
- Total Requests: {m.total_requests}
- Successful: {m.successful_requests}
- Failed: {m.failed_requests}
- Error Rate: {m.error_rate*100:.2f}%

**Latency Distribution:**
- Min: {m.min_response_time*1000:.2f}ms
- Average: {m.avg_response_time*1000:.2f}ms
- P50 (Median): {m.p50_latency*1000:.2f}ms
- P95: {m.p95_latency*1000:.2f}ms
- P99: {m.p99_latency*1000:.2f}ms
- Max: {m.max_response_time*1000:.2f}ms

**Throughput:**
- Requests/Second: {m.requests_per_second:.2f}
- Total Time: {m.total_time:.2f}s

**Event Loop Health:**
- Blocked: {"Yes" if m.event_loop_blocked else "No"}

---

"""

        # Add success criteria evaluation
        report += "\n## Success Criteria Evaluation\n\n"

        all_p95_under_2s = all(m.p95_latency < 2.0 for m in metrics)
        no_blocking = all(not m.event_loop_blocked for m in metrics)
        low_errors = all(m.error_rate < 0.01 for m in metrics)

        report += f"- ‚úÖ P95 Latency < 2s: {'PASS' if all_p95_under_2s else 'FAIL'}\n"
        report += f"- ‚úÖ No Event Loop Blocking: {'PASS' if no_blocking else 'FAIL'}\n"
        report += f"- ‚úÖ Error Rate < 1%: {'PASS' if low_errors else 'FAIL'}\n"

        if baseline:
            report += "\n## Before/After Comparison\n\n"
            report += "| Metric | Before (Sync) | After (Async) | Improvement |\n"
            report += "|--------|---------------|---------------|-------------|\n"

            for m in metrics:
                if m.test_name in baseline:
                    before = baseline[m.test_name]
                    improvement = (before - m.avg_response_time) / before * 100
                    report += f"| {m.test_name} | {before*1000:.2f}ms | {m.avg_response_time*1000:.2f}ms | {improvement:.1f}% |\n"

        report += "\n---\n\n*Generated by AutoBot Performance Testing Suite*\n"

        return report


class ChatLoadTestUser(HttpUser):
    """Locust user for chat endpoint load testing"""
    wait_time = between(0.1, 0.5)
    host = "http://172.16.168.20:8001"  # AutoBot backend

    @task
    def send_chat_message(self):
        """Send chat message to test concurrent load"""
        payload = {
            "message": "What is AutoBot?",
            "session_id": f"load_test_{self.environment.runner.user_count}_{time.time()}"
        }

        with self.client.post("/api/chat", json=payload, catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Got status code {response.status_code}")


class AsyncOperationsLoadTester:
    """Main load testing orchestrator for async operations"""

    def __init__(self):
        self.redis_manager: Optional[AsyncRedisManager] = None
        self.chat_manager: Optional[ChatWorkflowManager] = None
        self.metrics: List[PerformanceMetrics] = []

    async def setup(self):
        """Setup test environment"""
        logger.info("Setting up async load testing environment...")

        # Initialize Redis manager
        self.redis_manager = await get_redis_manager()

        # Initialize chat workflow manager
        self.chat_manager = ChatWorkflowManager()
        await self.chat_manager.initialize()

        logger.info("‚úÖ Setup complete")

    async def teardown(self):
        """Cleanup test environment"""
        logger.info("Cleaning up test environment...")

        if self.chat_manager:
            await self.chat_manager.shutdown()

        logger.info("‚úÖ Teardown complete")

    def _calculate_percentiles(self, response_times: List[float]) -> Dict[str, float]:
        """Calculate latency percentiles"""
        if not response_times:
            return {"p50": 0, "p95": 0, "p99": 0}

        return {
            "p50": np.percentile(response_times, 50),
            "p95": np.percentile(response_times, 95),
            "p99": np.percentile(response_times, 99)
        }

    async def _monitor_event_loop(self) -> bool:
        """Monitor event loop for blocking behavior"""
        blocked = False

        async def check_responsiveness():
            nonlocal blocked
            for _ in range(10):
                start = time.time()
                await asyncio.sleep(0.001)  # 1ms sleep
                duration = time.time() - start

                # If sleep takes >10ms, event loop is blocked
                if duration > 0.01:
                    blocked = True
                    logger.warning(f"Event loop blocked! Sleep took {duration*1000:.2f}ms")
                    return

        await check_responsiveness()
        return blocked

    async def test_concurrent_chat_requests(self, num_requests: int = 50) -> PerformanceMetrics:
        """
        Test 1: 50 Concurrent Chat Requests via HTTP Load Testing

        Uses Locust to simulate real HTTP load on chat endpoint.
        """
        logger.info(f"üöÄ Test 1: Running {num_requests} concurrent chat requests...")

        response_times = []
        successful = 0
        failed = 0

        # Run Locust load test programmatically
        env = Environment(user_classes=[ChatLoadTestUser])

        # Configure runner
        runner = env.create_local_runner()

        # Start load test
        start_time = time.time()
        runner.start(user_count=num_requests, spawn_rate=10)

        # Run for defined duration or until requests complete
        test_duration = 30  # 30 seconds
        await asyncio.sleep(test_duration)

        # Stop load test
        runner.stop()
        total_time = time.time() - start_time

        # Collect statistics from Locust
        stats = runner.stats.total

        response_times = [stat.response_time / 1000.0 for stat in runner.stats.entries.values()]  # Convert ms to seconds
        successful = stats.num_requests - stats.num_failures
        failed = stats.num_failures

        percentiles = self._calculate_percentiles(response_times)

        # Monitor event loop during cleanup
        event_loop_blocked = await self._monitor_event_loop()

        metrics = PerformanceMetrics(
            test_name="50 Concurrent Chat Requests (HTTP)",
            total_requests=stats.num_requests,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            avg_response_time=stats.avg_response_time / 1000.0 if stats.num_requests > 0 else 0,
            min_response_time=stats.min_response_time / 1000.0 if stats.min_response_time else 0,
            max_response_time=stats.max_response_time / 1000.0 if stats.max_response_time else 0,
            p50_latency=percentiles["p50"],
            p95_latency=percentiles["p95"],
            p99_latency=percentiles["p99"],
            requests_per_second=stats.total_rps,
            event_loop_blocked=event_loop_blocked,
            error_rate=failed / stats.num_requests if stats.num_requests > 0 else 0,
            response_times=response_times
        )

        logger.info(f"‚úÖ Test 1 Complete: {successful}/{stats.num_requests} successful, P95: {percentiles['p95']*1000:.2f}ms")

        return metrics

    async def test_concurrent_redis_operations(self, num_operations: int = 100) -> PerformanceMetrics:
        """
        Test 2: 100 Concurrent Redis Operations

        Tests async Redis operations under heavy concurrent load.
        """
        logger.info(f"üöÄ Test 2: Running {num_operations} concurrent Redis operations...")

        response_times = []
        successful = 0
        failed = 0

        # Get Redis database
        redis_db = await self.redis_manager.main()

        async def redis_operation(op_id: int):
            """Single Redis operation"""
            start = time.time()
            try:
                # Write operation
                await redis_db.set(f"load_test_key_{op_id}", f"value_{op_id}", ex=60)

                # Read operation
                value = await redis_db.get(f"load_test_key_{op_id}")

                duration = time.time() - start
                return {"success": True, "duration": duration}
            except Exception as e:
                logger.error(f"Redis operation {op_id} failed: {e}")
                duration = time.time() - start
                return {"success": False, "duration": duration}

        # Monitor event loop during operations
        event_loop_monitor = asyncio.create_task(self._monitor_event_loop())

        # Execute concurrent operations
        start_time = time.time()
        results = await asyncio.gather(*[redis_operation(i) for i in range(num_operations)])
        total_time = time.time() - start_time

        # Wait for event loop monitor
        event_loop_blocked = await event_loop_monitor

        # Process results
        for result in results:
            if result["success"]:
                successful += 1
            else:
                failed += 1
            response_times.append(result["duration"])

        percentiles = self._calculate_percentiles(response_times)

        metrics = PerformanceMetrics(
            test_name="100 Concurrent Redis Operations",
            total_requests=num_operations,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            p50_latency=percentiles["p50"],
            p95_latency=percentiles["p95"],
            p99_latency=percentiles["p99"],
            requests_per_second=num_operations / total_time if total_time > 0 else 0,
            event_loop_blocked=event_loop_blocked,
            error_rate=failed / num_operations if num_operations > 0 else 0,
            response_times=response_times
        )

        logger.info(f"‚úÖ Test 2 Complete: {successful}/{num_operations} successful, P95: {percentiles['p95']*1000:.2f}ms")

        return metrics

    async def test_mixed_file_redis_operations(self, num_operations: int = 50) -> PerformanceMetrics:
        """
        Test 3: Mixed File I/O and Redis Operations

        Tests 25 file operations + 25 Redis operations concurrently.
        """
        logger.info(f"üöÄ Test 3: Running {num_operations} mixed file/Redis operations...")

        response_times = []
        successful = 0
        failed = 0

        redis_db = await self.redis_manager.main()
        test_file_dir = Path("data/test_performance")
        test_file_dir.mkdir(parents=True, exist_ok=True)

        async def file_operation(op_id: int):
            """Async file I/O operation"""
            start = time.time()
            try:
                test_file = test_file_dir / f"test_{op_id}.txt"

                # Write operation
                async def write_file():
                    with open(test_file, 'w') as f:
                        f.write(f"Test data {op_id}")

                await asyncio.to_thread(write_file)

                # Read operation
                async def read_file():
                    with open(test_file, 'r') as f:
                        return f.read()

                await asyncio.to_thread(read_file)

                duration = time.time() - start
                return {"success": True, "duration": duration, "type": "file"}
            except Exception as e:
                logger.error(f"File operation {op_id} failed: {e}")
                duration = time.time() - start
                return {"success": False, "duration": duration, "type": "file"}

        async def redis_operation(op_id: int):
            """Async Redis operation"""
            start = time.time()
            try:
                await redis_db.set(f"mixed_test_{op_id}", f"value_{op_id}", ex=60)
                await redis_db.get(f"mixed_test_{op_id}")

                duration = time.time() - start
                return {"success": True, "duration": duration, "type": "redis"}
            except Exception as e:
                logger.error(f"Redis operation {op_id} failed: {e}")
                duration = time.time() - start
                return {"success": False, "duration": duration, "type": "redis"}

        # Monitor event loop
        event_loop_monitor = asyncio.create_task(self._monitor_event_loop())

        # Execute mixed operations concurrently
        file_ops = [file_operation(i) for i in range(num_operations // 2)]
        redis_ops = [redis_operation(i) for i in range(num_operations // 2, num_operations)]

        start_time = time.time()
        results = await asyncio.gather(*(file_ops + redis_ops))
        total_time = time.time() - start_time

        event_loop_blocked = await event_loop_monitor

        # Process results
        for result in results:
            if result["success"]:
                successful += 1
            else:
                failed += 1
            response_times.append(result["duration"])

        percentiles = self._calculate_percentiles(response_times)

        metrics = PerformanceMetrics(
            test_name="Mixed File I/O + Redis Operations",
            total_requests=num_operations,
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            p50_latency=percentiles["p50"],
            p95_latency=percentiles["p95"],
            p99_latency=percentiles["p99"],
            requests_per_second=num_operations / total_time if total_time > 0 else 0,
            event_loop_blocked=event_loop_blocked,
            error_rate=failed / num_operations if num_operations > 0 else 0,
            response_times=response_times
        )

        logger.info(f"‚úÖ Test 3 Complete: {successful}/{num_operations} successful, P95: {percentiles['p95']*1000:.2f}ms")

        return metrics

    async def test_cross_vm_concurrent_requests(self) -> PerformanceMetrics:
        """
        Test 4: Cross-VM Concurrent Requests

        Simulates requests from all 6 VMs simultaneously.
        """
        logger.info("üöÄ Test 4: Running cross-VM concurrent requests...")

        response_times = []
        successful = 0
        failed = 0

        # VM endpoints
        vms = {
            "main": "http://172.16.168.20:8001",
            "frontend": "http://172.16.168.21:5173",
            "npu_worker": "http://172.16.168.22:8081",
            "redis": "http://172.16.168.23:6379",
            "ai_stack": "http://172.16.168.24:8080",
            "browser": "http://172.16.168.25:3000"
        }

        async def vm_health_check(vm_name: str, vm_url: str):
            """Check VM health endpoint"""
            start = time.time()
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    # Try health endpoint
                    response = await client.get(f"{vm_url}/api/health")

                    duration = time.time() - start
                    return {
                        "success": response.status_code == 200,
                        "duration": duration,
                        "vm": vm_name
                    }
            except Exception as e:
                logger.warning(f"VM {vm_name} unreachable: {e}")
                duration = time.time() - start
                return {
                    "success": False,
                    "duration": duration,
                    "vm": vm_name
                }

        # Monitor event loop
        event_loop_monitor = asyncio.create_task(self._monitor_event_loop())

        # Execute concurrent VM checks
        start_time = time.time()
        results = await asyncio.gather(*[
            vm_health_check(vm_name, vm_url)
            for vm_name, vm_url in vms.items()
        ])
        total_time = time.time() - start_time

        event_loop_blocked = await event_loop_monitor

        # Process results
        for result in results:
            if result["success"]:
                successful += 1
            else:
                failed += 1
            response_times.append(result["duration"])

        percentiles = self._calculate_percentiles(response_times)

        metrics = PerformanceMetrics(
            test_name="Cross-VM Concurrent Requests",
            total_requests=len(vms),
            successful_requests=successful,
            failed_requests=failed,
            total_time=total_time,
            avg_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            p50_latency=percentiles["p50"],
            p95_latency=percentiles["p95"],
            p99_latency=percentiles["p99"],
            requests_per_second=len(vms) / total_time if total_time > 0 else 0,
            event_loop_blocked=event_loop_blocked,
            error_rate=failed / len(vms) if len(vms) > 0 else 0,
            response_times=response_times
        )

        logger.info(f"‚úÖ Test 4 Complete: {successful}/{len(vms)} VMs reachable, P95: {percentiles['p95']*1000:.2f}ms")

        return metrics

    async def run_all_tests(self) -> List[PerformanceMetrics]:
        """Run all performance load tests"""
        logger.info("=" * 80)
        logger.info("Starting Async Operations Load Testing Suite")
        logger.info("=" * 80)

        await self.setup()

        try:
            # Test 1: Concurrent Chat Requests
            metrics_1 = await self.test_concurrent_chat_requests(num_requests=50)
            self.metrics.append(metrics_1)

            # Test 2: Concurrent Redis Operations
            metrics_2 = await self.test_concurrent_redis_operations(num_operations=100)
            self.metrics.append(metrics_2)

            # Test 3: Mixed File/Redis Operations
            metrics_3 = await self.test_mixed_file_redis_operations(num_operations=50)
            self.metrics.append(metrics_3)

            # Test 4: Cross-VM Requests
            metrics_4 = await self.test_cross_vm_concurrent_requests()
            self.metrics.append(metrics_4)

        finally:
            await self.teardown()

        logger.info("=" * 80)
        logger.info("Load Testing Suite Complete")
        logger.info("=" * 80)

        return self.metrics


# Pytest test cases
@pytest.mark.asyncio
async def test_50_concurrent_chat_requests():
    """Pytest wrapper for Test 1: 50 Concurrent Chat Requests"""
    tester = AsyncOperationsLoadTester()
    await tester.setup()

    try:
        metrics = await tester.test_concurrent_chat_requests(num_requests=50)

        # Assert success criteria
        assert metrics.p95_latency < 2.0, f"P95 latency {metrics.p95_latency:.2f}s exceeds 2s threshold"
        assert not metrics.event_loop_blocked, "Event loop was blocked during test"
        assert metrics.error_rate < 0.01, f"Error rate {metrics.error_rate*100:.2f}% exceeds 1% threshold"

        logger.info(f"‚úÖ Test passed: {metrics.test_name}")
    finally:
        await tester.teardown()


@pytest.mark.asyncio
async def test_100_concurrent_redis_operations():
    """Pytest wrapper for Test 2: 100 Concurrent Redis Operations"""
    tester = AsyncOperationsLoadTester()
    await tester.setup()

    try:
        metrics = await tester.test_concurrent_redis_operations(num_operations=100)

        # Assert success criteria
        assert metrics.total_time < 2.0, f"Total time {metrics.total_time:.2f}s exceeds 2s threshold"
        assert not metrics.event_loop_blocked, "Event loop was blocked during test"
        assert metrics.error_rate < 0.01, f"Error rate {metrics.error_rate*100:.2f}% exceeds 1% threshold"

        logger.info(f"‚úÖ Test passed: {metrics.test_name}")
    finally:
        await tester.teardown()


@pytest.mark.asyncio
async def test_mixed_file_redis_operations():
    """Pytest wrapper for Test 3: Mixed File I/O + Redis Operations"""
    tester = AsyncOperationsLoadTester()
    await tester.setup()

    try:
        metrics = await tester.test_mixed_file_redis_operations(num_operations=50)

        # Assert success criteria
        assert not metrics.event_loop_blocked, "Event loop was blocked during test"
        assert metrics.error_rate < 0.01, f"Error rate {metrics.error_rate*100:.2f}% exceeds 1% threshold"
        assert metrics.total_time < 5.0, f"Total time {metrics.total_time:.2f}s exceeds 5s threshold"

        logger.info(f"‚úÖ Test passed: {metrics.test_name}")
    finally:
        await tester.teardown()


@pytest.mark.asyncio
async def test_cross_vm_concurrent_requests():
    """Pytest wrapper for Test 4: Cross-VM Concurrent Requests"""
    tester = AsyncOperationsLoadTester()
    await tester.setup()

    try:
        metrics = await tester.test_cross_vm_concurrent_requests()

        # Assert at least 50% of VMs are reachable
        success_rate = metrics.successful_requests / metrics.total_requests
        assert success_rate >= 0.5, f"Only {success_rate*100:.1f}% of VMs reachable"
        assert not metrics.event_loop_blocked, "Event loop was blocked during test"

        logger.info(f"‚úÖ Test passed: {metrics.test_name}")
    finally:
        await tester.teardown()


# Main execution
if __name__ == "__main__":
    async def main():
        """Run all tests and generate report"""
        tester = AsyncOperationsLoadTester()
        metrics = await tester.run_all_tests()

        # Generate performance report
        report_gen = PerformanceReportGenerator()
        report_paths = report_gen.generate_report(metrics)

        logger.info("\n" + "=" * 80)
        logger.info("Performance Report Generated:")
        logger.info(f"  JSON: {report_paths['json_path']}")
        logger.info(f"  Markdown: {report_paths['markdown_path']}")
        logger.info("=" * 80)

        # Print summary
        print("\nüìä PERFORMANCE TEST SUMMARY")
        print("=" * 80)
        for m in metrics:
            print(f"\n{m.test_name}:")
            print(f"  ‚úÖ Successful: {m.successful_requests}/{m.total_requests}")
            print(f"  ‚è±Ô∏è  P95 Latency: {m.p95_latency*1000:.2f}ms")
            print(f"  üîÑ Throughput: {m.requests_per_second:.2f} req/s")
            print(f"  üö® Error Rate: {m.error_rate*100:.2f}%")
            print(f"  ‚ö° Event Loop: {'BLOCKED ‚ùå' if m.event_loop_blocked else 'OK ‚úÖ'}")
        print("=" * 80)

    asyncio.run(main())
