---
# AI/ML VM Configuration (VM4)
# Services: AI Stack, NPU Worker, Intel OpenVINO, Model serving

# ==========================================
# FIREWALL CONFIGURATION (Issue #894)
# ==========================================

security:
  # AI/ML-specific firewall rules (least privilege)
  firewall_rules:
    # AI Stack service (infrastructure only)
    - rule: allow
      port: 8080
      proto: tcp
      src: "172.16.168.0/24"
      comment: "AI Stack API"
    # NPU Worker service (infrastructure only)
    - rule: allow
      port: 8081
      proto: tcp
      src: "172.16.168.0/24"
      comment: "NPU Worker API"
    # Ollama API (infrastructure only)
    - rule: allow
      port: 11434
      proto: tcp
      src: "172.16.168.0/24"
      comment: "Ollama API"

  # Model security
  model_security:
    # Model verification
    verify_checksums: true
    scan_models: true

    # Access control
    model_permissions:
      owner: "{{ autobot.user }}"
      group: "{{ autobot.group }}"
      files: "644"
      directories: "755"

  # API security
  api_security:
    rate_limiting: true
    input_validation: true
    output_sanitization: true

    # CORS configuration
    cors_origins:
      - "http://172.16.168.21"  # Frontend VM
      - "http://172.16.168.20:8001"  # Backend VM

# ==========================================
# AI STACK CONFIGURATION
# ==========================================

ai_stack:
  # Service configuration
  service:
    name: "autobot-ai-stack"
    host: "0.0.0.0"
    port: 8080
    workers: 2
    worker_class: "uvicorn.workers.UvicornWorker"
    timeout: 120

  # Application settings
  app:
    module: "ai_api_server:app"
    working_directory: "{{ autobot.base_dir }}/ai-stack"
    log_level: "INFO"

  # Model configuration
  models:
    base_path: "/var/lib/autobot/models"
    cache_path: "/var/lib/autobot/models/cache"
    temp_path: "/tmp/autobot/models"

  # Environment variables
  environment:
    PYTHONPATH: "{{ autobot.base_dir }}"
    AI_SERVER_HOST: "0.0.0.0"
    AI_SERVER_PORT: 8080
    AI_SERVER_WORKERS: 2
    REDIS_HOST: "{{ redis_host }}"
    REDIS_PORT: "{{ redis_port }}"
    CHROMA_DB_PATH: "/var/lib/autobot/chroma_db"
    DISABLE_EXTERNAL_APIS: "false"  # Enable external APIs in production
    TF_USE_LEGACY_KERAS: 1
    KERAS_BACKEND: "tensorflow"

    # OpenVINO configuration
    OPENVINO_DEVICE: "AUTO"
    OV_LOG_LEVEL: "ERROR"
    LD_LIBRARY_PATH: "/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib/python3.10"

    # GPU configuration (if available)
    NVIDIA_VISIBLE_DEVICES: "all"
    NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

# ==========================================
# NPU WORKER CONFIGURATION
# ==========================================

npu_worker:
  # Service configuration
  service:
    name: "autobot-npu-worker"
    host: "0.0.0.0"
    port: 8081
    workers: 1  # NPU typically single-threaded
    timeout: 60

  # Application settings
  app:
    module: "npu_worker_main:app"
    working_directory: "{{ autobot.base_dir }}/npu-worker"
    log_level: "INFO"

  # Environment variables
  environment:
    PYTHONPATH: "{{ autobot.base_dir }}"
    NPU_WORKER_HOST: "0.0.0.0"
    NPU_WORKER_PORT: 8081
    NPU_WORKER_WORKERS: 1
    REDIS_HOST: "{{ redis_host }}"
    REDIS_PORT: "{{ redis_port }}"
    NPU_LOG_LEVEL: "INFO"

    # Intel NPU configuration
    OPENVINO_DEVICE: "NPU"
    OV_LOG_LEVEL: "ERROR"

    # Performance optimization
    OMP_NUM_THREADS: 4
    MKL_NUM_THREADS: 4

# ==========================================
# INTEL OPENVINO CONFIGURATION
# ==========================================

openvino:
  # Installation configuration
  version: "2024.4"
  install_path: "/opt/intel/openvino"

  # Package repositories
  repository:
    key_url: "https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB"
    repo_url: "deb https://apt.repos.intel.com/openvino/2024 {{ ansible_distribution_release }} main"

  # Packages to install
  packages:
    - "openvino-toolkit"
    - "openvino-dev-utils"
    - "openvino-samples"

  # Runtime configuration
  runtime:
    # Device priorities for inference
    device_priority: "NPU,GPU,CPU"

    # Performance hints
    performance_hint: "THROUGHPUT"  # or "LATENCY"

    # Optimization level
    optimization_level: "PERFORMANCE"

  # Model optimization
  model_optimizer:
    enabled: true
    precision: "FP16"  # Use FP16 for NPU optimization
    input_shape: "auto"

# ==========================================
# SYSTEM PACKAGES
# ==========================================

packages:
  system:
    # Build essentials
    - build-essential
    - cmake
    - pkg-config
    - git
    - curl
    - wget

    # Python development
    - python3
    - python3-pip
    - python3-venv
    - python3-dev

    # Intel NPU drivers and utilities
    - intel-level-zero-gpu
    - level-zero-dev
    - intel-opencl-icd

    # System monitoring
    - htop
    - iotop
    - nvidia-smi  # If NVIDIA GPU present
    - intel-gpu-tools

    # Media libraries (for vision models)
    - libopencv-dev
    - libjpeg-dev
    - libpng-dev
    - libglib2.0-dev

  python:
    # Core AI/ML packages
    - torch>=2.0.0
    - torchvision>=0.15.0
    - transformers>=4.55.2
    - sentence-transformers>=2.2.0
    - openai>=1.0.0
    - anthropic>=0.7.0

    # Intel OpenVINO Python API
    - openvino>=2024.4.0
    - openvino-dev>=2024.4.0

    # Computer vision
    - opencv-python>=4.8.0
    - pillow>=10.0.0
    - numpy>=1.24.0

    # Scientific computing
    - scipy>=1.11.0
    - scikit-learn>=1.3.0

    # Async and web framework
    - fastapi>=0.115.0
    - uvicorn>=0.30.0
    - aiohttp>=3.12.15

# ==========================================
# GPU/NPU HARDWARE CONFIGURATION
# ==========================================

hardware:
  # NPU configuration
  npu:
    enabled: true
    device_check: "lspci | grep -i 'neural\\|npu\\|ai'"
    driver_check: "ls /dev/accel*"

    # NPU-specific optimizations
    optimizations:
      batch_size: 1  # NPU typically optimized for single inference
      precision: "FP16"
      threading: false  # NPU handles threading internally

  # GPU configuration (if available)
  gpu:
    enabled: true
    nvidia_enabled: true
    intel_gpu_enabled: true

    # GPU detection commands
    nvidia_check: "nvidia-smi"
    intel_check: "intel_gpu_top -l"

    # GPU optimizations
    optimizations:
      cuda_visible_devices: "all"
      mixed_precision: true
      batch_size: 4  # Larger batches for GPU

  # CPU fallback configuration
  cpu:
    threads: 4
    optimization_flags: "-O3 -march=native"

# ==========================================
# MODEL MANAGEMENT
# ==========================================

model_management:
  # Model categories and configurations
  categories:
    language_models:
      path: "/var/lib/autobot/models/llm"
      formats: ["gguf", "safetensors", "bin"]
      optimization_target: "npu"  # Prefer NPU for language models

    embedding_models:
      path: "/var/lib/autobot/models/embeddings"
      formats: ["safetensors", "bin", "onnx"]
      optimization_target: "gpu"  # GPU good for embeddings

    vision_models:
      path: "/var/lib/autobot/models/vision"
      formats: ["onnx", "pb", "tflite"]
      optimization_target: "npu"  # NPU optimized for vision

    audio_models:
      path: "/var/lib/autobot/models/audio"
      formats: ["onnx", "wav2vec2"]
      optimization_target: "cpu"  # CPU fallback for audio

  # Automatic model optimization
  auto_optimization:
    enabled: true
    target_precision: "FP16"
    batch_optimization: true

  # Model caching strategy
  caching:
    enabled: true
    cache_size: "10gb"
    eviction_policy: "lru"
    preload_common_models: true

# ==========================================
# SYSTEMD SERVICE CONFIGURATION
# ==========================================

systemd_services:
  # AI Stack Service
  - name: "autobot-ai-stack"
    description: "AutoBot AI Stack Service"
    type: "simple"
    user: "{{ autobot.user }}"
    group: "{{ autobot.group }}"
    working_directory: "{{ ai_stack.app.working_directory }}"
    exec_start: "{{ autobot.base_dir }}/venv/bin/uvicorn {{ ai_stack.app.module }} --host {{ ai_stack.service.host }} --port {{ ai_stack.service.port }} --workers {{ ai_stack.service.workers }}"
    restart: "always"
    restart_sec: 15
    environment_file: "/etc/autobot/ai-stack.env"

    # Service dependencies
    wants:
      - "network-online.target"
      - "redis.service"
    after:
      - "network-online.target"
      - "redis.service"
    requires:
      - "network-online.target"

    # Resource limits
    limit_nofile: 65536
    limit_nproc: 4096
    memory_max: "12G"  # Limit memory for AI workloads

  # NPU Worker Service
  - name: "autobot-npu-worker"
    description: "AutoBot NPU Worker Service"
    type: "simple"
    user: "{{ autobot.user }}"
    group: "{{ autobot.group }}"
    working_directory: "{{ npu_worker.app.working_directory }}"
    exec_start: "{{ autobot.base_dir }}/venv/bin/uvicorn {{ npu_worker.app.module }} --host {{ npu_worker.service.host }} --port {{ npu_worker.service.port }} --workers {{ npu_worker.service.workers }}"
    restart: "always"
    restart_sec: 15
    environment_file: "/etc/autobot/npu-worker.env"

    # Service dependencies
    wants:
      - "network-online.target"
      - "redis.service"
    after:
      - "network-online.target"
      - "redis.service"
    requires:
      - "network-online.target"

    # Hardware access
    device_policy: "strict"
    device_allow:
      - "/dev/accel* rw"  # NPU device access
      - "/dev/dri* rw"    # GPU device access

  # Model Optimization Service
  - name: "autobot-model-optimizer"
    description: "AutoBot Model Optimization Service"
    type: "oneshot"
    user: "{{ autobot.user }}"
    group: "{{ autobot.group }}"
    exec_start: "/opt/autobot/scripts/optimize-models.sh"

    # Timer configuration (runs daily)
    timer:
      on_calendar: "daily"
      persistent: true

# ==========================================
# FIREWALL CONFIGURATION
# ==========================================

ufw_rules:
  # AI Stack API
  - rule: allow
    port: 8080
    protocol: tcp
    src: "172.16.168.0/24"
    comment: "AI Stack API"

  # NPU Worker API
  - rule: allow
    port: 8081
    protocol: tcp
    src: "172.16.168.0/24"
    comment: "NPU Worker API"

  # Model serving endpoints
  # TODO Issue #725: Remove unused port range via SLM config
  - rule: allow
    port: "8090:8099"
    protocol: tcp
    src: "172.16.168.0/24"
    comment: "Model serving endpoints"

# ==========================================
# MONITORING CONFIGURATION
# ==========================================

monitoring:
  # Hardware monitoring
  hardware_monitoring:
    enabled: true

    # NPU monitoring
    npu_metrics:
      - "utilization"
      - "memory_usage"
      - "temperature"
      - "power_consumption"

    # GPU monitoring (if available)
    gpu_metrics:
      - "utilization"
      - "memory_usage"
      - "temperature"
      - "power_draw"

  # Service monitoring
  service_monitoring:
    # Health check endpoints
    health_checks:
      - path: "/health"
        port: 8080
        service: "ai-stack"
        expected_status: 200

      - path: "/health"
        port: 8081
        service: "npu-worker"
        expected_status: 200

    # Performance metrics
    performance_metrics:
      - "inference_latency"
      - "throughput_requests_per_second"
      - "model_load_time"
      - "queue_depth"

  # Resource monitoring
  resource_monitoring:
    thresholds:
      cpu_usage: 80      # percent
      memory_usage: 85   # percent
      gpu_memory: 90     # percent
      disk_usage: 80     # percent

# ==========================================
# PERFORMANCE OPTIMIZATION
# ==========================================

performance:
  # System-level optimizations
  system:
    # CPU governor for AI workloads
    cpu_governor: "performance"

    # Memory optimization
    transparent_hugepages: "always"  # Good for AI workloads
    swappiness: 10  # Reduce swapping

    # I/O scheduler optimization
    io_scheduler: "mq-deadline"  # Good for SSD storage

  # AI-specific optimizations
  ai_optimization:
    # Thread configuration
    omp_num_threads: 4
    mkl_num_threads: 4

    # Memory pool optimization
    pytorch_memory_fraction: 0.8

    # Model compilation optimization
    torch_compile: true
    openvino_auto_batching: true

# ==========================================
# BACKUP CONFIGURATION
# ==========================================

backup_paths:
  # Model files (large, less frequent backup)
  - path: "/var/lib/autobot/models"
    type: "directory"
    frequency: "weekly"
    compress: true
    exclude:
      - "cache/*"
      - "temp/*"
      - "*.tmp"

  # AI Stack configuration
  - path: "/etc/autobot/ai-stack"
    type: "directory"
    frequency: "daily"
    compress: true

  # OpenVINO configuration
  - path: "/opt/intel/openvino/etc"
    type: "directory"
    frequency: "weekly"
    compress: true

  # Application logs
  - path: "/var/log/autobot/ai-ml"
    type: "directory"
    frequency: "daily"
    compress: true
    retention: 30  # days

# ==========================================
# TROUBLESHOOTING CONFIGURATION (continued after security section)
# ==========================================

# ==========================================
# TROUBLESHOOTING CONFIGURATION
# ==========================================

troubleshooting:
  # Debug tools
  debug_tools:
    - "intel_gpu_top"
    - "nvidia-smi"
    - "htop"
    - "iotop"
    - "lsof"
    - "strace"

  # Log verbosity
  debug_logging:
    enabled: false  # Set to true for troubleshooting
    log_level: "DEBUG"

  # Hardware diagnostics
  hardware_diagnostics:
    # NPU diagnostics
    npu_test: "python3 -c 'import openvino as ov; print(ov.Core().available_devices)'"

    # GPU diagnostics
    gpu_test: "nvidia-smi -L"  # or "intel_gpu_top -l" for Intel GPU

    # Model loading test
    model_test: "/opt/autobot/scripts/test-model-loading.py"
