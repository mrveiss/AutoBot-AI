#!/usr/bin/env python3
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""
Comprehensive Code Quality Analysis Dashboard
Runs all analyzers and provides unified quality metrics and recommendations

NOTE: run_comprehensive_quality_analysis (~145 lines) is an ACCEPTABLE EXCEPTION
per Issue #490 - analysis dashboard with sequential report generation. Low priority.
"""

import asyncio
import json
from pathlib import Path

from code_quality_dashboard import CodeQualityDashboard


def _print_executive_metrics(metrics: dict, issues: dict, report: dict) -> None:
    """
    Print executive-level quality metrics summary.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print(f"ğŸ“Š **Overall Quality Assessment:**")  # noqa: print
    print(f"   ğŸ¯ Overall Quality Score: {metrics['overall_score']}/100")  # noqa: print
    print(f"   ğŸ“‹ Total Issues Found: {issues['total_issues']}")  # noqa: print
    print(f"   ğŸš¨ Critical Issues: {issues['critical_issues']}")  # noqa: print
    print(
        f"   âš ï¸  High Priority Issues: {issues['high_priority_issues']}"
    )  # noqa: print
    print(f"   ğŸ“ Files Analyzed: {report['files_analyzed']}")  # noqa: print
    print(
        f"   â±ï¸  Analysis Time: {report['analysis_time_seconds']:.2f} seconds"
    )  # noqa: print
    print()  # noqa: print

    # Category breakdown
    print("ğŸ·ï¸  **Issues by Category:**")  # noqa: print
    for category, count in issues["by_category"].items():
        category_name = category.replace("_", " ").title()
        print(f"   â€¢ {category_name}: {count} issues")  # noqa: print
    print()  # noqa: print


def _print_analyzer_scores(metrics: dict) -> None:
    """
    Print individual analyzer scores with visual indicators.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸ” **Individual Analysis Scores:**")  # noqa: print
    score_categories = [
        ("Security", metrics["security_score"], "ğŸ›¡ï¸"),
        ("Performance", metrics["performance_score"], "âš¡"),
        ("Architecture", metrics["architecture_score"], "ğŸ—ï¸"),
        ("Test Coverage", metrics["test_coverage_score"], "ğŸ§ª"),
        ("API Consistency", metrics["api_consistency_score"], "ğŸ”—"),
        ("Code Duplication", metrics["code_duplication_score"], "â™»ï¸"),
        ("Environment Config", metrics["environment_config_score"], "âš™ï¸"),
    ]

    for name, score, emoji in score_categories:
        status = get_score_status(score)
        status_color = get_status_emoji(score)
        print(f"   {emoji} {name}: {score}/100 {status_color} {status}")  # noqa: print
    print()  # noqa: print


def _print_technical_debt(debt: dict) -> None:
    """
    Print technical debt analysis and effort breakdown.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸ’¸ **Technical Debt Analysis:**")  # noqa: print
    print(  # noqa: print
        f"   ğŸ“Š Total Estimated Effort: {debt['estimated_total_effort_days']} days ({debt['estimated_total_effort_hours']} hours)"
    )
    print(  # noqa: print
        f"   ğŸš¨ Critical Issues Effort: {debt['estimated_critical_effort_hours']} hours"
    )
    print(f"   ğŸ“ˆ Debt Ratio: {debt['debt_ratio']}% of total project")  # noqa: print
    print()  # noqa: print

    print("ğŸ’° **Effort by Category:**")  # noqa: print
    for category, data in debt["effort_by_category"].items():
        category_name = category.replace("_", " ").title()
        print(  # noqa: print
            f"   â€¢ {category_name}: {data['count']} issues, {data['effort_hours']} hours"
        )
    print()  # noqa: print


def _print_priority_issues(report: dict) -> None:
    """
    Print top priority issues requiring immediate action.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    print("ğŸš¨ **Top Priority Issues (Immediate Action Required):**")  # noqa: print
    critical_issues = [
        issue
        for issue in report["prioritized_issues"]
        if issue["severity"] == "critical"
    ]
    high_issues = [
        issue for issue in report["prioritized_issues"] if issue["severity"] == "high"
    ]
    top_issues = critical_issues[:5] + high_issues[:5]

    for i, issue in enumerate(top_issues[:10], 1):
        severity_emoji = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
        emoji = severity_emoji.get(issue["severity"], "âšª")

        print(
            f"\n{i}. {emoji} **{issue['title']}** ({issue['severity'].upper()})"
        )  # noqa: print
        print(
            f"   ğŸ“‚ Category: {issue['category'].replace('_', ' ').title()}"
        )  # noqa: print
        if issue["file_path"] != "Multiple files":
            print(
                f"   ğŸ“„ File: {issue['file_path']}:{issue['line_number']}"
            )  # noqa: print
        print(f"   ğŸ“ Description: {issue['description']}")  # noqa: print
        print(f"   ğŸ’¡ Fix: {issue['fix_suggestion']}")  # noqa: print
        print(f"   ğŸ”§ Effort: {issue['estimated_effort'].title()}")  # noqa: print
        print(f"   ğŸ¯ Priority Score: {issue['priority_score']}/100")  # noqa: print
    print()  # noqa: print


def _print_analysis_alerts(report: dict) -> None:
    """
    Print critical alerts for security, performance, and testing.

    Issue #281: Extracted from run_comprehensive_quality_analysis to reduce
    function length and improve readability of dashboard output sections.
    """
    # Security-specific analysis
    if report["detailed_analyses"].get("security"):
        security_data = report["detailed_analyses"]["security"]
        if security_data.get("critical_vulnerabilities", 0) > 0:
            print("ğŸ›¡ï¸ **CRITICAL SECURITY ALERT:**")  # noqa: print
            print(  # noqa: print
                f"   Found {security_data['critical_vulnerabilities']} critical security vulnerabilities!"
            )
            print(
                "   These must be addressed immediately before deployment."
            )  # noqa: print
            print()  # noqa: print

    # Performance-specific analysis
    if report["detailed_analyses"].get("performance"):
        perf_data = report["detailed_analyses"]["performance"]
        if perf_data.get("critical_issues", 0) > 0:
            print("âš¡ **CRITICAL PERFORMANCE ALERT:**")  # noqa: print
            print(  # noqa: print
                f"   Found {perf_data['critical_issues']} critical performance issues!"
            )
            print(
                "   These may cause memory leaks or system instability."
            )  # noqa: print
            print()  # noqa: print

    # Testing coverage analysis
    if report["detailed_analyses"].get("testing_coverage"):
        test_data = report["detailed_analyses"]["testing_coverage"]
        coverage = test_data.get("test_coverage_percentage", 0)
        print(f"ğŸ§ª **Testing Coverage Analysis:**")  # noqa: print
        print(f"   Current test coverage: {coverage}%")  # noqa: print
        if coverage < 70:
            print("   âš ï¸  Coverage is below recommended 70% threshold")  # noqa: print
            print("   Consider adding more unit and integration tests")  # noqa: print
        print()  # noqa: print


def _print_analysis_intro() -> None:
    """Print the analyzer startup banner. Issue #1183: Extracted from run_comprehensive_quality_analysis()."""
    print("ğŸ¯ Starting comprehensive code quality analysis...")  # noqa: print
    print("This will run all available analyzers:")  # noqa: print
    print("  â€¢ Code Duplication Analyzer")  # noqa: print
    print("  â€¢ Environment Variable Analyzer")  # noqa: print
    print("  â€¢ Performance & Memory Leak Analyzer")  # noqa: print
    print("  â€¢ Security Vulnerability Analyzer")  # noqa: print
    print("  â€¢ API Consistency Analyzer")  # noqa: print
    print("  â€¢ Testing Coverage Gap Analyzer")  # noqa: print
    print("  â€¢ Architectural Pattern Analyzer")  # noqa: print
    print()  # noqa: print


def _print_quality_trends(report: dict) -> None:
    """Print the quality trends section. Issue #1183: Extracted from run_comprehensive_quality_analysis()."""
    if not report.get("quality_trends"):
        return
    print("ğŸ“ˆ **Quality Trends:**")  # noqa: print
    trends = report["quality_trends"]
    if len(trends) > 1:
        latest = trends[0]
        previous = trends[1]
        score_change = latest["overall_score"] - previous["overall_score"]
        trend_emoji = "ğŸ“ˆ" if score_change > 0 else "ğŸ“‰" if score_change < 0 else "â¡ï¸"
        print(
            f"   {trend_emoji} Score change: {score_change:+.1f} points"
        )  # noqa: print
        issue_change = latest["issue_count"] - previous["issue_count"]
        issue_emoji = "ğŸ“‰" if issue_change < 0 else "ğŸ“ˆ" if issue_change > 0 else "â¡ï¸"
        print(f"   {issue_emoji} Issue count change: {issue_change:+d}")  # noqa: print
    else:
        print("   ğŸ“Š Baseline measurement established")  # noqa: print
    print()  # noqa: print


# Issue #1183: Module-level constant extracted from _print_detailed_summaries() to reduce function length
_ANALYSIS_FORMATS = {
    "duplication": {
        "name": "Code Duplication",
        "emoji": "â™»ï¸",
        "fields": [
            ("total_duplicate_groups", "Found {} duplicate code groups"),
            ("total_lines_saved", "Potential lines saved: {}"),
        ],
    },
    "environment": {
        "name": "Environment Variables",
        "emoji": "âš™ï¸",
        "fields": [
            ("total_hardcoded_values", "Found {} hardcoded values"),
            ("critical_hardcoded_values", "Critical values: {}"),
        ],
    },
    "security": {
        "name": "Security",
        "emoji": "ğŸ›¡ï¸",
        "fields": [
            ("total_vulnerabilities", "Found {} potential vulnerabilities"),
            ("critical_vulnerabilities", "Critical vulnerabilities: {}"),
        ],
    },
    "performance": {
        "name": "Performance",
        "emoji": "âš¡",
        "fields": [
            ("total_performance_issues", "Found {} performance issues"),
            ("critical_issues", "Critical issues: {}"),
        ],
    },
    "api_consistency": {
        "name": "API Consistency",
        "emoji": "ğŸ”—",
        "fields": [
            ("total_endpoints", "Analyzed {} API endpoints"),
            ("inconsistencies_found", "Found {} consistency issues"),
        ],
    },
    "testing_coverage": {
        "name": "Testing Coverage",
        "emoji": "ğŸ§ª",
        "fields": [
            ("total_functions", "Analyzed {} functions"),
            ("test_coverage_percentage", "Test coverage: {}%"),
        ],
    },
    "architecture": {
        "name": "Architecture",
        "emoji": "ğŸ—ï¸",
        "fields": [
            ("total_components", "Analyzed {} architectural components"),
            ("architectural_issues", "Found {} architectural issues"),
        ],
    },
}


def _print_detailed_summaries(report: dict) -> None:
    """Print per-analyzer summary blocks.

    Issue #1183: Extracted from run_comprehensive_quality_analysis();
    analysis_formats dict further extracted to _ANALYSIS_FORMATS constant.
    """
    print("ğŸ“Š **Detailed Analysis Summaries:**")  # noqa: print
    for analysis_type, fmt in _ANALYSIS_FORMATS.items():
        data = report["detailed_analyses"].get(analysis_type)
        if data:
            print(f"\n{fmt['emoji']} **{fmt['name']} Analysis:**")  # noqa: print
            for field_key, field_fmt in fmt["fields"]:
                value = data.get(field_key, 0)
                print(f"   â€¢ {field_fmt.format(value)}")  # noqa: print


async def run_comprehensive_quality_analysis():
    """Run comprehensive code quality analysis"""

    # Issue #1183: Delegate intro printing to extracted helper
    _print_analysis_intro()

    dashboard = CodeQualityDashboard()

    # Generate comprehensive report
    report = await dashboard.generate_comprehensive_report(
        root_path=".", patterns=["src/**/*.py", "backend/**/*.py"], include_trends=True
    )

    print("=== Code Quality Executive Summary ===\n")  # noqa: print

    # Executive summary
    summary = await dashboard.generate_executive_summary(report)
    print(summary)  # noqa: print

    print("\n=== Detailed Quality Analysis Results ===\n")  # noqa: print

    # Issue #281: Use extracted helpers for dashboard sections
    metrics = report["quality_metrics"]
    issues = report["issue_summary"]
    debt = report["technical_debt"]

    _print_executive_metrics(metrics, issues, report)
    _print_analyzer_scores(metrics)
    _print_technical_debt(debt)
    _print_priority_issues(report)
    _print_analysis_alerts(report)

    # Improvement recommendations
    print("ğŸ“‹ **Improvement Recommendations (Priority Order):**")  # noqa: print
    for i, recommendation in enumerate(report["improvement_recommendations"], 1):
        print(f"{i}. {recommendation}")  # noqa: print
    print()  # noqa: print

    # Issue #1183: Delegate trends/summaries to extracted helpers
    _print_quality_trends(report)
    _print_detailed_summaries(report)

    # Save comprehensive report
    report_path = Path("comprehensive_quality_report.json")
    with open(report_path, "w") as f:
        json.dump(report, f, indent=2, default=str)

    # Generate summary report
    summary_path = Path("quality_summary.txt")
    with open(summary_path, "w") as f:
        f.write(summary)

    print(f"\n=== Reports Generated ===")  # noqa: print
    print(f"ğŸ“‹ Comprehensive report: {report_path}")  # noqa: print
    print(f"ğŸ“„ Executive summary: {summary_path}")  # noqa: print

    return report


def get_score_status(score: float) -> str:
    """Get human-readable status for score"""
    if score >= 90:
        return "EXCELLENT"
    elif score >= 80:
        return "GOOD"
    elif score >= 70:
        return "FAIR"
    elif score >= 60:
        return "NEEDS IMPROVEMENT"
    else:
        return "CRITICAL"


def get_status_emoji(score: float) -> str:
    """Get status emoji based on score"""
    if score >= 90:
        return "ğŸŸ¢"
    elif score >= 80:
        return "ğŸŸ¡"
    elif score >= 70:
        return "ğŸŸ "
    else:
        return "ğŸ”´"


async def generate_action_plan(report):
    """Generate specific action plan based on results"""

    print("\n=== ğŸ“‹ Recommended Action Plan ===")  # noqa: print

    metrics = report["quality_metrics"]
    issues = report["issue_summary"]

    # Phase 1: Critical Issues (Week 1)
    critical_count = issues["critical_issues"]
    if critical_count > 0:
        print(f"\nğŸš¨ **Phase 1: Critical Issues (IMMEDIATE - Week 1)**")  # noqa: print
        print(f"   Address {critical_count} critical issues:")  # noqa: print

        critical_issues = [
            i for i in report["prioritized_issues"] if i["severity"] == "critical"
        ]
        for issue in critical_issues[:5]:  # Top 5 critical
            print(f"   â€¢ {issue['title']}")  # noqa: print
            print(f"     Action: {issue['fix_suggestion']}")  # noqa: print

    # Phase 2: High Priority (Weeks 2-3)
    high_count = issues["high_priority_issues"]
    if high_count > 0:
        print(f"\nâš ï¸  **Phase 2: High Priority (Weeks 2-3)**")  # noqa: print
        print(f"   Address {high_count} high priority issues:")  # noqa: print

        if metrics["security_score"] < 80:
            print("   â€¢ Complete security vulnerability audit")  # noqa: print
        if metrics["performance_score"] < 70:
            print("   â€¢ Fix performance bottlenecks and memory leaks")  # noqa: print
        if metrics["test_coverage_score"] < 70:
            print("   â€¢ Increase test coverage to 80%+")  # noqa: print

    # Phase 3: Quality Improvements (Month 2)
    print(f"\nğŸ”§ **Phase 3: Quality Improvements (Month 2)**")  # noqa: print
    if metrics["architecture_score"] < 80:
        print("   â€¢ Refactor architectural issues")  # noqa: print
    if metrics["code_duplication_score"] < 80:
        print("   â€¢ Eliminate code duplication")  # noqa: print
    if metrics["api_consistency_score"] < 80:
        print("   â€¢ Standardize API patterns")  # noqa: print

    # Phase 4: Maintenance & Monitoring (Ongoing)
    print(f"\nğŸ“ˆ **Phase 4: Continuous Improvement (Ongoing)**")  # noqa: print
    print("   â€¢ Set up automated quality monitoring")  # noqa: print
    print("   â€¢ Implement pre-commit quality checks")  # noqa: print
    print("   â€¢ Regular quality reviews (weekly)")  # noqa: print
    print("   â€¢ Update team coding standards")  # noqa: print

    # Estimated timeline
    debt = report["technical_debt"]
    total_days = debt["estimated_total_effort_days"]
    critical_hours = debt["estimated_critical_effort_hours"]

    print(f"\nâ° **Estimated Timeline:**")  # noqa: print
    print(f"   â€¢ Critical fixes: {critical_hours} hours (1-2 weeks)")  # noqa: print
    print(
        f"   â€¢ Total remediation: {total_days} days ({total_days/5:.1f} weeks)"
    )  # noqa: print
    print(f"   â€¢ Team of 2-3 developers recommended")  # noqa: print


async def main():
    """Run comprehensive code quality analysis"""

    # Run analysis
    report = await run_comprehensive_quality_analysis()

    # Generate action plan
    await generate_action_plan(report)

    print("\n=== ğŸ¯ Analysis Complete ===")  # noqa: print
    print("Next Steps:")  # noqa: print
    print(
        "1. Review comprehensive_quality_report.json for detailed findings"
    )  # noqa: print
    print("2. Start with critical security and performance issues")  # noqa: print
    print("3. Follow the recommended action plan phases")  # noqa: print
    print("4. Set up automated quality monitoring")  # noqa: print
    print("5. Schedule regular quality reviews")  # noqa: print


if __name__ == "__main__":
    asyncio.run(main())
