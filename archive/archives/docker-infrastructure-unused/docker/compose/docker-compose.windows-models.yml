# Docker Compose using Windows Ollama Models Only
# All containers and WSL use models from Windows directory

name: autobot

services:
  # Option 1: Direct mount of Windows models (if you want container Ollama)
  autobot-ollama-windows:
    image: ollama/ollama:latest
    container_name: autobot-ollama-windows
    restart: unless-stopped
    ports:
      - "${AUTOBOT_OLLAMA_PORT:-11434}:11434"
    volumes:
      # Mount Windows Ollama directory
      # Adjust the username as needed
      - /mnt/c/Users/${WINDOWS_USERNAME:-marti}/.ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - autobot-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Redis Stack
  autobot-redis:
    image: redis/redis-stack:7.4.0-v1
    container_name: autobot-redis
    restart: unless-stopped
    ports:
      - "${AUTOBOT_REDIS_PORT:-6379}:6379"
      - "${AUTOBOT_REDIS_INSIGHT_PORT:-8002}:8002"
    volumes:
      - autobot_redis_data:/data
    environment:
      - REDIS_ARGS=--appendonly yes --save 60 1
    networks:
      - autobot-network

  # AI Stack Container
  autobot-ai-stack:
    build:
      context: ../..
      dockerfile: docker/ai-stack/Dockerfile
    container_name: autobot-ai-stack
    restart: unless-stopped
    ports:
      - "${AUTOBOT_AI_STACK_PORT:-8080}:8080"
    volumes:
      - ../volumes/prompts:/app/prompts:ro
      - ../volumes/knowledge_base:/app/knowledge_base:ro
      - autobot_ai_data:/app/data
    environment:
      - AI_SERVER_HOST=0.0.0.0
      - AI_SERVER_PORT=8080
      # Use Windows Ollama directly (no container needed)
      - OLLAMA_HOST=${AUTOBOT_OLLAMA_HOST:-127.0.0.2}:${AUTOBOT_OLLAMA_PORT:-11434}
      - REDIS_HOST=autobot-redis
      - REDIS_PORT=${AUTOBOT_REDIS_PORT:-6379}
    depends_on:
      - autobot-redis
    networks:
      - autobot-network
    extra_hosts:
      # Allow container to reach Windows host
      - "host.docker.internal:host-gateway"

  # NPU Worker
  autobot-npu-worker:
    build:
      context: ../..
      dockerfile: docker/npu-worker/Dockerfile
    container_name: autobot-npu-worker
    restart: unless-stopped
    ports:
      - "${AUTOBOT_NPU_WORKER_PORT:-8081}:8081"
    volumes:
      - ../volumes/prompts:/app/prompts:ro
      - autobot_npu_logs:/app/logs
    environment:
      - NPU_WORKER_HOST=0.0.0.0
      - NPU_WORKER_PORT=8081
      # Use Windows Ollama directly
      - OLLAMA_HOST=${AUTOBOT_OLLAMA_HOST:-127.0.0.2}:${AUTOBOT_OLLAMA_PORT:-11434}
      - REDIS_HOST=autobot-redis
      - REDIS_PORT=${AUTOBOT_REDIS_PORT:-6379}
    depends_on:
      - autobot-redis
    networks:
      - autobot-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

networks:
  autobot-network:
    driver: bridge
    name: autobot-network

volumes:
  autobot_redis_data:
    name: autobot_redis_data
  autobot_ai_data:
    name: autobot_ai_data
  autobot_npu_logs:
    name: autobot_npu_logs
