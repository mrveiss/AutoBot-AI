paramiko>=4.0.0

# TensorFlow compatibility - protobuf 4.x required for TensorFlow 2.19.1
# Protobuf 5.x/6.x causes MessageFactory.GetPrototype AttributeError
protobuf>=4.23.0,<5.0.0

# vLLM - High-performance LLM inference with prefix caching (Optional)
# Provides 3-4x throughput improvement with 98.7% cache efficiency
# Requires: CUDA-compatible GPU, Enable in config: llm.vllm.enabled = true
# Installation: pip install vllm (or pip install -r requirements.txt)
vllm>=0.6.0
