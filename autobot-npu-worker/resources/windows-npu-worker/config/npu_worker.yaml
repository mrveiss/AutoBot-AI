# AutoBot NPU Worker Configuration - Windows Deployment
# This configuration is for the optional Windows NPU worker installation
# Separate from the VM-based NPU worker on 172.16.168.22:8081

# Service Configuration
service:
  # Service binding configuration
  host: "0.0.0.0"  # Bind to all network interfaces for remote access
  port: 8082       # Windows worker uses 8082 (VM worker uses 8081)

  # Worker identification
  worker_id_prefix: "windows_npu_worker"

  # Service behavior
  workers: 1       # Number of uvicorn workers
  reload: false    # Disable auto-reload in production
  log_level: "info"

# Backend Integration
backend:
  # Main AutoBot backend server
  host: "172.16.168.20"
  port: 8001
  protocol: "http"

  # Service registry
  register_with_backend: true
  registration_interval: 30  # Seconds between registration updates
  health_check_interval: 60  # Seconds between health checks

  # Timeout settings
  timeout: 30
  connect_timeout: 10

# Redis Configuration
# NOTE: Redis credentials are fetched from backend via bootstrap endpoint
# No credentials should be hardcoded here - worker gets them on first contact
redis:
  # Connection settings (host/port/password come from bootstrap)
  # These are fallbacks only if bootstrap fails
  host: null      # Fetched from backend
  port: 6379      # Default port
  password: null  # NEVER hardcode - fetched from backend

  # Database selection
  db: 0  # Main database for task queues

  # Connection pool settings
  max_connections: 20
  socket_timeout: 5
  socket_connect_timeout: 2
  retry_on_timeout: true

  # Task queue names
  queues:
    pending: "npu_tasks_pending"
    processing: "npu_tasks_processing"
    completed: "npu_tasks_completed"
    failed: "npu_tasks_failed"

# NPU Optimization Settings
# Issue #640: Uses ONNX Runtime + OpenVINO EP for proper Intel NPU support.
# DirectML doesn't expose Intel NPUs - OpenVINO EP has explicit NPU device option.
npu:
  # Hardware acceleration priority
  enabled: true
  fallback_to_cpu: true  # Use CPU if NPU/GPU not available

  # ONNX Runtime OpenVINO Execution Provider optimization
  optimization:
    precision: "INT8"           # NPU works best with INT8
    batch_size: 32              # Optimal NPU batch size
    num_streams: 2              # NPU execution streams
    num_threads: 4              # NPU worker threads
    inference_precision: "INT8"
    execution_mode: "NPU_FAST"
    cache_enabled: true
    batch_optimization: true

  # Device selection priority
  # GPU.1 = NVIDIA RTX 4070 (fastest for embeddings)
  # GPU.0 = Intel Arc Graphics (iGPU)
  # NPU = Intel AI Boost (power efficient but slower)
  # CPU = Intel Core Ultra 9 (fallback)
  device_priority:
    - "GPU.1"   # NVIDIA RTX 4070 - ~10-100x faster than NPU for embeddings
    - "GPU.0"   # Intel Arc Graphics - faster than NPU
    - "NPU"     # Intel AI Boost - power efficient but slower
    - "GPU"     # Generic GPU fallback
    - "CPU"     # CPU fallback

  # Issue #165: Parallel device configuration
  # Use different devices for different workloads for maximum throughput
  parallel_devices:
    enabled: true
    # Embedding workloads: Use NVIDIA GPU for maximum speed
    embedding_device: "GPU.1"
    # Chat/inference workloads: Can use NPU (power efficient) or GPU
    chat_device: "NPU"
    # Fallback if preferred device unavailable
    fallback_device: "CPU"

  # Model cache
  model_cache_dir: "models"
  model_cache_size_gb: 5

# Default Models
models:
  # Embedding model
  embedding:
    name: "nomic-embed-text"
    type: "embedding"
    optimization_level: "speed"
    preload: true

  # Chat model
  chat:
    name: "llama3.2:1b-instruct-q4_K_M"
    type: "chat"
    optimization_level: "balanced"
    preload: true

  # Model loading
  autoload_defaults: true
  load_timeout: 300  # Seconds

# Performance Settings
performance:
  # Task processing
  max_concurrent_tasks: 3
  task_timeout: 30  # Seconds

  # Caching
  embedding_cache:
    enabled: true
    max_size: 1000  # Number of cached embeddings
    ttl: 3600       # Time to live in seconds

  # Batch processing
  batch_processing:
    enabled: true
    max_batch_size: 32
    batch_timeout: 0.1  # Seconds to wait for batch to fill

# Logging Configuration
logging:
  # Log level
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL

  # Log directories
  directory: "logs"
  service_log: "logs/service.log"
  app_log: "logs/app.log"
  error_log: "logs/error.log"

  # Log rotation
  max_size_mb: 100
  backup_count: 5

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"

  # Structured logging
  use_json_format: false
  include_thread_id: true
  include_process_id: true

# Security Settings
security:
  # API security
  require_authentication: false  # Set to true for production
  api_key: null  # Set API key if authentication enabled

  # Network security
  allowed_hosts:
    - "172.16.168.20"  # Backend
    - "172.16.168.21"  # Frontend
    - "172.16.168.22"  # VM NPU worker
    - "172.16.168.23"  # Redis
    - "172.16.168.24"  # AI Stack
    - "172.16.168.25"  # Browser service
    - "localhost"
    - "127.0.0.1"

  # Request limits
  rate_limiting:
    enabled: false
    requests_per_minute: 100

# Monitoring Settings
monitoring:
  # Metrics collection
  enabled: true
  collection_interval: 5  # Seconds

  # Health checks
  health_check:
    enabled: true
    check_npu: true
    check_redis: true
    check_models: true

  # Performance tracking
  track_performance: true
  performance_history_size: 1000

# Windows-Specific Settings
windows:
  # Service configuration
  service_name: "AutoBotNPUWorker"
  display_name: "AutoBot NPU Worker (Windows)"
  description: "AutoBot optional NPU worker for hardware-accelerated AI processing"

  # Startup configuration
  startup_type: "automatic"  # automatic, manual, disabled
  delayed_start: true

  # Recovery options
  recovery:
    restart_on_failure: true
    restart_delay: 60  # Seconds
    max_restart_attempts: 3
    reset_period: 86400  # Seconds (24 hours)

# Development Settings
development:
  # Debug mode
  debug: false
  verbose_logging: false

  # Testing
  mock_npu: false  # Use mock NPU for testing without hardware
  benchmark_on_startup: false

# Feature Flags
features:
  # Enable/disable specific features
  semantic_search: true
  embedding_generation: true
  chat_inference: true
  model_optimization: true
  performance_benchmarking: true

  # Experimental features
  experimental:
    batch_inference: false
    dynamic_batching: false
    model_quantization: false

# Environment
environment: "production"  # development, testing, production
