#!/usr/bin/env python3
"""
Test script to verify LLM connection and model loading fixes
"""

import requests
import json
import sys

BASE_URL = "http://localhost:8001"

def test_endpoint(name, url, expected_keys=None):
    """Test an endpoint and check response"""
    print(f"\nğŸ§ª Testing {name}: {url}")
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            print(f"âŒ Status: {response.status_code}")
            return False
        
        data = response.json()
        print(f"âœ… Status: {response.status_code}")
        
        if expected_keys:
            for key in expected_keys:
                if key in data:
                    value = data[key]
                    if isinstance(value, bool):
                        status = "âœ…" if value else "âŒ"
                    elif isinstance(value, str):
                        status = "âœ…" if value else "âŒ (empty)"
                    elif isinstance(value, (list, dict)):
                        status = f"âœ… ({len(value)} items)" if value else "âŒ (empty)"
                    else:
                        status = f"âœ… ({value})"
                    print(f"   {key}: {status}")
                else:
                    print(f"   {key}: âŒ (missing)")
        
        return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def test_comprehensive_status():
    """Test the comprehensive LLM status"""
    print(f"\nğŸ§ª Testing Comprehensive LLM Status")
    try:
        response = requests.get(f"{BASE_URL}/api/llm/status/comprehensive", timeout=10)
        if response.status_code != 200:
            print(f"âŒ Status: {response.status_code}")
            return False
        
        data = response.json()
        print(f"âœ… Status: {response.status_code}")
        
        # Check structure
        provider_type = data.get("provider_type", "unknown")
        print(f"   Provider Type: âœ… {provider_type}")
        
        if provider_type == "local":
            ollama_config = data.get("providers", {}).get("local", {}).get("ollama", {})
            configured = ollama_config.get("configured", False)
            model = ollama_config.get("model", "")
            status = "âœ…" if configured and model else "âŒ"
            print(f"   Ollama Configured: {status}")
            print(f"   Ollama Model: {'âœ…' if model else 'âŒ'} ({model})")
        
        return True
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def main():
    print("ğŸ” Testing LLM Connection and Model Loading Fixes")
    print("=" * 50)
    
    # Test endpoints
    tests = [
        ("System Health", f"{BASE_URL}/api/system/health", 
         ["llm_status", "current_model", "embedding_status", "current_embedding_model"]),
        
        ("LLM Status", f"{BASE_URL}/api/llm/status", 
         ["status", "model", "provider_type"]),
        
        ("LLM Models", f"{BASE_URL}/api/llm/models", 
         ["models", "total_count"]),
        
        ("Agent Config", f"{BASE_URL}/api/agent-config/agents", 
         ["agents", "total_count"]),
    ]
    
    results = []
    for name, url, keys in tests:
        success = test_endpoint(name, url, keys)
        results.append((name, success))
    
    # Test comprehensive status separately
    comprehensive_success = test_comprehensive_status()
    results.append(("Comprehensive LLM Status", comprehensive_success))
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“Š Test Results Summary:")
    all_passed = True
    for name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"   {name}: {status}")
        if not success:
            all_passed = False
    
    print(f"\nğŸ¯ Overall Result: {'âœ… ALL TESTS PASSED' if all_passed else 'âŒ SOME TESTS FAILED'}")
    
    if all_passed:
        print("\nğŸ‰ LLM connection and model loading fixes are working!")
        print("   The Settings Panel should now show:")
        print("   - LLM Status: Connected âœ…")
        print("   - Current Model: llama3.2:1b-instruct-q4_K_M âœ…")
        print("   - Model Dropdown: Populated with available models âœ…")
    else:
        print("\nâš ï¸ Some issues remain. Check the failing tests above.")
    
    return 0 if all_passed else 1

if __name__ == "__main__":
    sys.exit(main())