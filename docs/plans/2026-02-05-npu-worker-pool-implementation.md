# NPU Multi-Worker Pool Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement health-aware load balancing across multiple NPU workers with automatic failover and circuit breaker protection.

**Architecture:** Extends existing `NPUWorkerClient` and `NPUTaskQueue` to support multiple workers transparently. Pool manages worker selection using priority-first + least-connections algorithm, monitors health via hybrid approach (background + on-demand), and implements conservative circuit breaker pattern.

**Tech Stack:** Python 3.10+, asyncio, aiohttp, YAML config, pytest

**Issue:** #168

---

## Phase 1: Configuration & Core Data Structures

### Task 1: Add CircuitState Enum and WorkerState Dataclass

**Files:**
- Modify: `src/npu_integration.py:1-50` (add after existing imports)

**Step 1: Write the failing test**

Create: `tests/test_npu_worker_pool.py`

```python
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""Tests for NPU Worker Pool (#168)"""

import pytest
from src.npu_integration import CircuitState, WorkerState, NPUWorkerClient


def test_circuit_state_enum_values():
    """Circuit breaker should have three states"""
    assert CircuitState.CLOSED.value == "closed"
    assert CircuitState.OPEN.value == "open"
    assert CircuitState.HALF_OPEN.value == "half_open"


def test_worker_state_initialization():
    """WorkerState should initialize with correct defaults"""
    client = NPUWorkerClient("http://test:8081")
    state = WorkerState(worker_id="test-1", client=client)

    assert state.worker_id == "test-1"
    assert state.client == client
    assert state.active_tasks == 0
    assert state.total_requests == 0
    assert state.failures == 0
    assert state.healthy is True
    assert state.circuit_state == CircuitState.CLOSED
    assert state.circuit_open_until == 0
```

**Step 2: Run test to verify it fails**

```bash
cd /home/kali/Desktop/AutoBot
pytest tests/test_npu_worker_pool.py::test_circuit_state_enum_values -v
```

Expected: FAIL with "ImportError: cannot import name 'CircuitState'"

**Step 3: Write minimal implementation**

Modify: `src/npu_integration.py` (add after line 16, before NPUInferenceRequest)

```python
from enum import Enum


class CircuitState(Enum):
    """Circuit breaker states for worker health management"""

    CLOSED = "closed"  # Normal operation
    OPEN = "open"  # Worker failed, blocking requests
    HALF_OPEN = "half_open"  # Testing recovery


@dataclass
class WorkerState:
    """
    Track per-worker metrics and health status (Issue #168).

    Manages active task count, failure tracking, and circuit breaker state
    for load balancing and failover.
    """

    worker_id: str
    client: NPUWorkerClient
    active_tasks: int = 0
    total_requests: int = 0
    failures: int = 0
    last_health_check: float = 0
    healthy: bool = True
    circuit_state: CircuitState = CircuitState.CLOSED
    circuit_open_until: float = 0
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_circuit_state_enum_values -v
pytest tests/test_npu_worker_pool.py::test_worker_state_initialization -v
```

Expected: Both PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): add CircuitState enum and WorkerState dataclass (#168)

- Add CircuitState enum with CLOSED/OPEN/HALF_OPEN states
- Add WorkerState dataclass for tracking worker metrics
- Initialize with safe defaults for circuit breaker pattern

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 2: Add Configuration Loader

**Files:**
- Modify: `src/npu_integration.py` (add before NPUWorkerClient class)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
import os
import tempfile
import yaml


def test_load_worker_config_success():
    """Should load and parse valid worker configuration"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://172.16.168.22:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            },
            {
                "id": "worker-2",
                "url": "http://172.16.168.20:8082",
                "priority": 5,
                "enabled": False,
                "max_concurrent_tasks": 2,
            },
        ],
        "load_balancing": {
            "health_check_interval": 30,
            "retry_cooldown_seconds": 60,
        },
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import load_worker_config

        result = load_worker_config(config_path)

        assert len(result["workers"]) == 2
        assert result["workers"][0]["id"] == "worker-1"
        assert result["workers"][0]["priority"] == 10
        assert result["load_balancing"]["health_check_interval"] == 30
    finally:
        os.unlink(config_path)


def test_load_worker_config_missing_file():
    """Should return default config when file missing"""
    from src.npu_integration import load_worker_config

    result = load_worker_config("nonexistent.yaml")

    assert "workers" in result
    assert len(result["workers"]) == 0
    assert result["load_balancing"]["health_check_interval"] == 30


def test_load_worker_config_validation():
    """Should validate required fields"""
    config_data = {
        "workers": [
            {"id": "worker-1", "url": "http://test:8081"}  # Missing priority, enabled
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import load_worker_config

        with pytest.raises(ValueError, match="Missing required field"):
            load_worker_config(config_path)
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_load_worker_config_success -v
```

Expected: FAIL with "ImportError: cannot import name 'load_worker_config'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (before NPUWorkerClient class)

```python
def load_worker_config(config_path: str = "config/npu_workers.yaml") -> Dict[str, Any]:
    """
    Load and validate NPU worker configuration from YAML.

    Args:
        config_path: Path to worker configuration file

    Returns:
        Dict containing workers list and load_balancing config

    Raises:
        ValueError: If required fields missing or invalid

    Issue #168: Configuration loader for multi-worker pool
    """
    # Default configuration
    default_config = {
        "workers": [],
        "load_balancing": {
            "health_check_interval": 30,
            "retry_cooldown_seconds": 60,
            "retry_failed_workers": True,
            "strategy": "priority",
            "timeout_seconds": 10,
        },
    }

    # Return default if file doesn't exist
    if not os.path.exists(config_path):
        logger.warning(
            "NPU worker config not found at %s, using single worker mode", config_path
        )
        return default_config

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        if not config:
            return default_config

        # Validate workers
        workers = config.get("workers", [])
        for worker in workers:
            required_fields = ["id", "url", "priority", "enabled"]
            for field in required_fields:
                if field not in worker:
                    raise ValueError(
                        f"Missing required field '{field}' in worker config: {worker}"
                    )

            # Validate priority range
            if not 1 <= worker["priority"] <= 10:
                raise ValueError(
                    f"Worker priority must be 1-10, got {worker['priority']}"
                )

        # Merge with defaults
        result = default_config.copy()
        result["workers"] = workers
        if "load_balancing" in config:
            result["load_balancing"].update(config["load_balancing"])

        logger.info("Loaded %d NPU workers from %s", len(workers), config_path)
        return result

    except Exception as e:
        logger.error("Failed to load NPU worker config from %s: %s", config_path, e)
        raise
```

Add import at top:

```python
import os
import yaml
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_load_worker_config_success -v
pytest tests/test_npu_worker_pool.py::test_load_worker_config_missing_file -v
pytest tests/test_npu_worker_pool.py::test_load_worker_config_validation -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): add worker configuration loader (#168)

- Load worker config from YAML with validation
- Validate required fields and priority range
- Return defaults when file missing
- Add comprehensive error handling

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 2: NPU Worker Pool Core

### Task 3: Implement NPUWorkerPool Initialization

**Files:**
- Modify: `src/npu_integration.py` (add after NPUTaskQueue class)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_npu_worker_pool_initialization():
    """Pool should initialize with workers from config"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://172.16.168.22:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ],
        "load_balancing": {"health_check_interval": 30},
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        assert len(pool.workers) == 1
        assert "worker-1" in pool.workers
        assert pool.workers["worker-1"].worker_id == "worker-1"
        assert pool.workers["worker-1"].active_tasks == 0
        assert pool.config["load_balancing"]["health_check_interval"] == 30
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_npu_worker_pool_fallback_single_worker():
    """Pool should fall back to single worker when no config"""
    from src.npu_integration import NPUWorkerPool

    pool = NPUWorkerPool(config_path="nonexistent.yaml")
    await pool.initialize()

    # Should create default worker from service registry
    assert len(pool.workers) >= 0  # May be 0 or 1 depending on service registry
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_npu_worker_pool_initialization -v
```

Expected: FAIL with "ImportError: cannot import name 'NPUWorkerPool'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (after NPUTaskQueue class)

```python
class NPUWorkerPool:
    """
    Manages pool of NPU workers with load balancing and health monitoring.

    Implements priority-first + least-connections algorithm for worker selection,
    hybrid health monitoring (background + on-demand), and conservative circuit
    breaker pattern for fault tolerance.

    Issue #168: Multi-worker load balancing and pool management
    """

    def __init__(self, config_path: str = "config/npu_workers.yaml"):
        """
        Initialize NPU worker pool.

        Args:
            config_path: Path to worker configuration YAML
        """
        self.config_path = config_path
        self.config: Dict[str, Any] = {}
        self.workers: Dict[str, WorkerState] = {}
        self._worker_lock = asyncio.Lock()
        self._health_monitor_task: Optional[asyncio.Task] = None
        self._initialized = False

    async def initialize(self):
        """
        Load configuration and initialize worker clients.

        Must be called before using the pool.
        """
        if self._initialized:
            return

        # Load configuration
        self.config = load_worker_config(self.config_path)

        # Initialize workers from config
        async with self._worker_lock:
            for worker_config in self.config["workers"]:
                if not worker_config["enabled"]:
                    continue

                worker_id = worker_config["id"]
                client = NPUWorkerClient(npu_endpoint=worker_config["url"])

                self.workers[worker_id] = WorkerState(
                    worker_id=worker_id,
                    client=client,
                )

            # Fallback to single worker if no workers configured
            if len(self.workers) == 0:
                logger.warning(
                    "No NPU workers configured, attempting single worker fallback"
                )
                try:
                    from src.utils.service_registry import get_service_url

                    fallback_url = get_service_url("npu-worker")
                    fallback_client = NPUWorkerClient(npu_endpoint=fallback_url)
                    self.workers["default"] = WorkerState(
                        worker_id="default",
                        client=fallback_client,
                    )
                    logger.info("Using fallback NPU worker at %s", fallback_url)
                except Exception as e:
                    logger.debug(
                        "Could not initialize fallback NPU worker: %s", e
                    )

        self._initialized = True
        logger.info("NPU worker pool initialized with %d workers", len(self.workers))
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_npu_worker_pool_initialization -v
pytest tests/test_npu_worker_pool.py::test_npu_worker_pool_fallback_single_worker -v
```

Expected: Both PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement NPUWorkerPool initialization (#168)

- Initialize pool from YAML configuration
- Create WorkerState for each enabled worker
- Fallback to single worker when no config
- Thread-safe initialization with asyncio lock

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 4: Implement Worker Selection Algorithm

**Files:**
- Modify: `src/npu_integration.py` (add method to NPUWorkerPool)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_worker_selection_priority_first():
    """Should select highest priority worker"""
    config_data = {
        "workers": [
            {
                "id": "low-priority",
                "url": "http://test1:8081",
                "priority": 5,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
            {
                "id": "high-priority",
                "url": "http://test2:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # High priority should be selected even with no load difference
        worker = await pool._select_worker(excluded_workers=set())
        assert worker.worker_id == "high-priority"
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_worker_selection_least_connections():
    """Should select least loaded worker within same priority"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
            {
                "id": "worker-2",
                "url": "http://test2:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Simulate load on worker-1
        pool.workers["worker-1"].active_tasks = 5
        pool.workers["worker-2"].active_tasks = 2

        # Should select worker-2 (less loaded)
        worker = await pool._select_worker(excluded_workers=set())
        assert worker.worker_id == "worker-2"
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_worker_selection_excludes_circuit_open():
    """Should skip workers with open circuit breaker"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
            {
                "id": "worker-2",
                "url": "http://test2:8081",
                "priority": 5,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool, CircuitState
        import time

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Open circuit on high-priority worker
        pool.workers["worker-1"].circuit_state = CircuitState.OPEN
        pool.workers["worker-1"].circuit_open_until = time.time() + 60

        # Should fall back to lower priority worker
        worker = await pool._select_worker(excluded_workers=set())
        assert worker.worker_id == "worker-2"
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_worker_selection_priority_first -v
```

Expected: FAIL with "AttributeError: 'NPUWorkerPool' object has no attribute '_select_worker'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (inside NPUWorkerPool class)

```python
async def _select_worker(
    self, excluded_workers: Optional[Set[str]] = None
) -> Optional[WorkerState]:
    """
    Select best worker using priority-first + least-connections algorithm.

    Args:
        excluded_workers: Set of worker IDs to exclude (for retry logic)

    Returns:
        WorkerState for selected worker, or None if no workers available

    Algorithm:
    1. Filter: enabled=true, circuit != OPEN, not excluded
    2. Group by priority (higher = better)
    3. Within highest priority group, select least loaded (fewest active_tasks)

    Issue #168: Priority-first + least-connections load balancing
    """
    if excluded_workers is None:
        excluded_workers = set()

    current_time = time.time()
    candidates = []

    async with self._worker_lock:
        for worker_id, worker_state in self.workers.items():
            # Skip excluded workers
            if worker_id in excluded_workers:
                continue

            # Skip workers with open circuit breaker
            if worker_state.circuit_state == CircuitState.OPEN:
                if current_time < worker_state.circuit_open_until:
                    continue
                # Circuit cooldown expired, move to half-open
                worker_state.circuit_state = CircuitState.HALF_OPEN
                logger.info(
                    "Circuit breaker for %s entering HALF_OPEN state", worker_id
                )

            # Add to candidates
            # Get worker priority from config
            worker_config = next(
                (w for w in self.config["workers"] if w["id"] == worker_id),
                {"priority": 1},
            )
            priority = worker_config.get("priority", 1)

            candidates.append((priority, worker_state.active_tasks, worker_state))

    if not candidates:
        logger.warning("No available NPU workers (all excluded or circuit open)")
        return None

    # Sort by priority DESC, then active_tasks ASC
    candidates.sort(key=lambda x: (-x[0], x[1]))

    selected = candidates[0][2]
    logger.debug(
        "Selected worker %s (priority=%d, active_tasks=%d)",
        selected.worker_id,
        candidates[0][0],
        selected.active_tasks,
    )

    return selected
```

Add import at top:

```python
import time
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_worker_selection_priority_first -v
pytest tests/test_npu_worker_pool.py::test_worker_selection_least_connections -v
pytest tests/test_npu_worker_pool.py::test_worker_selection_excludes_circuit_open -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement worker selection algorithm (#168)

- Priority-first selection (higher priority preferred)
- Least-connections tiebreaker within same priority
- Exclude workers with open circuit breakers
- Automatic transition to HALF_OPEN after cooldown

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 3: Health Monitoring & Circuit Breaker

### Task 5: Implement Health Check Methods

**Files:**
- Modify: `src/npu_integration.py` (add methods to NPUWorkerPool)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_check_worker_health_success(mocker):
    """Successful health check should mark worker healthy"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock successful health check
        mock_health = mocker.patch.object(
            pool.workers["worker-1"].client,
            "check_health",
            return_value={"status": "healthy"},
        )

        await pool._check_worker_health("worker-1")

        assert pool.workers["worker-1"].healthy is True
        assert pool.workers["worker-1"].failures == 0
        assert pool.workers["worker-1"].last_health_check > 0
        mock_health.assert_called_once()
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_check_worker_health_failure(mocker):
    """Failed health check should increment failure count"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock failed health check
        mock_health = mocker.patch.object(
            pool.workers["worker-1"].client,
            "check_health",
            return_value={"status": "unhealthy"},
        )

        await pool._check_worker_health("worker-1")

        assert pool.workers["worker-1"].healthy is False
        assert pool.workers["worker-1"].failures == 1
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_check_worker_health_success -v
```

Expected: FAIL with "AttributeError: 'NPUWorkerPool' object has no attribute '_check_worker_health'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (inside NPUWorkerPool class)

```python
async def _check_worker_health(self, worker_id: str) -> bool:
    """
    Check health of a specific worker.

    Args:
        worker_id: Worker identifier

    Returns:
        True if healthy, False otherwise

    Issue #168: On-demand health checking
    """
    if worker_id not in self.workers:
        return False

    worker_state = self.workers[worker_id]

    try:
        health_result = await worker_state.client.check_health()

        async with self._worker_lock:
            worker_state.last_health_check = time.time()

            if health_result.get("status") == "healthy":
                worker_state.healthy = True
                worker_state.failures = 0
                logger.debug("Worker %s health check: HEALTHY", worker_id)
                return True
            else:
                worker_state.healthy = False
                worker_state.failures += 1
                logger.debug(
                    "Worker %s health check: UNHEALTHY (failures=%d)",
                    worker_id,
                    worker_state.failures,
                )
                return False

    except Exception as e:
        async with self._worker_lock:
            worker_state.healthy = False
            worker_state.failures += 1
            worker_state.last_health_check = time.time()

        logger.debug("Worker %s health check failed: %s", worker_id, e)
        return False
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_check_worker_health_success -v
pytest tests/test_npu_worker_pool.py::test_check_worker_health_failure -v
```

Expected: Both PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement worker health checking (#168)

- Check individual worker health via /health endpoint
- Track failure count and last check timestamp
- Update healthy status based on response
- Thread-safe state updates with asyncio lock

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 6: Implement Circuit Breaker Logic

**Files:**
- Modify: `src/npu_integration.py` (add methods to NPUWorkerPool)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_circuit_breaker_opens_after_threshold():
    """Circuit should open after 5 consecutive failures"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool, CircuitState

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        worker_state = pool.workers["worker-1"]

        # Simulate 5 failures
        for _ in range(5):
            await pool._record_task_failure("worker-1", Exception("Test failure"))

        # Circuit should be open
        assert worker_state.circuit_state == CircuitState.OPEN
        assert worker_state.circuit_open_until > time.time()
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_circuit_breaker_resets_on_success():
    """Successful task should reset failure count"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool, CircuitState

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        worker_state = pool.workers["worker-1"]

        # Simulate some failures
        worker_state.failures = 3

        # Record success
        await pool._record_task_success("worker-1")

        # Failures should reset
        assert worker_state.failures == 0
        assert worker_state.circuit_state == CircuitState.CLOSED
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_circuit_breaker_half_open_closes_on_success():
    """HALF_OPEN circuit should close on successful test request"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool, CircuitState

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        worker_state = pool.workers["worker-1"]

        # Set to HALF_OPEN
        worker_state.circuit_state = CircuitState.HALF_OPEN
        worker_state.failures = 5

        # Record success
        await pool._record_task_success("worker-1")

        # Circuit should close
        assert worker_state.circuit_state == CircuitState.CLOSED
        assert worker_state.failures == 0
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_circuit_breaker_opens_after_threshold -v
```

Expected: FAIL with "AttributeError: 'NPUWorkerPool' object has no attribute '_record_task_failure'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (inside NPUWorkerPool class)

```python
async def _record_task_success(self, worker_id: str):
    """
    Record successful task execution.

    Resets failure count and closes circuit breaker if in HALF_OPEN state.

    Args:
        worker_id: Worker identifier

    Issue #168: Circuit breaker success handling
    """
    if worker_id not in self.workers:
        return

    async with self._worker_lock:
        worker_state = self.workers[worker_id]
        worker_state.failures = 0

        # Close circuit if in HALF_OPEN (test request succeeded)
        if worker_state.circuit_state == CircuitState.HALF_OPEN:
            worker_state.circuit_state = CircuitState.CLOSED
            logger.info("Circuit breaker for %s closed (recovery successful)", worker_id)


async def _record_task_failure(self, worker_id: str, error: Exception):
    """
    Record failed task execution.

    Increments failure count and opens circuit breaker after threshold (5 failures).

    Args:
        worker_id: Worker identifier
        error: Exception that caused the failure

    Issue #168: Circuit breaker failure handling
    """
    if worker_id not in self.workers:
        return

    async with self._worker_lock:
        worker_state = self.workers[worker_id]
        worker_state.failures += 1

        # Get circuit breaker config
        failure_threshold = 5  # Conservative: 5 failures
        cooldown_seconds = self.config["load_balancing"].get(
            "retry_cooldown_seconds", 60
        )

        # Open circuit if threshold exceeded
        if worker_state.failures >= failure_threshold:
            worker_state.circuit_state = CircuitState.OPEN
            worker_state.circuit_open_until = time.time() + cooldown_seconds
            logger.error(
                "Circuit breaker OPENED for %s after %d failures (cooldown=%ds)",
                worker_id,
                worker_state.failures,
                cooldown_seconds,
            )

        # Reopen circuit if HALF_OPEN test request failed
        elif worker_state.circuit_state == CircuitState.HALF_OPEN:
            worker_state.circuit_state = CircuitState.OPEN
            worker_state.circuit_open_until = time.time() + cooldown_seconds
            logger.warning(
                "Circuit breaker for %s reopened (test request failed)", worker_id
            )
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_circuit_breaker_opens_after_threshold -v
pytest tests/test_npu_worker_pool.py::test_circuit_breaker_resets_on_success -v
pytest tests/test_npu_worker_pool.py::test_circuit_breaker_half_open_closes_on_success -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement circuit breaker logic (#168)

- Open circuit after 5 consecutive failures
- Close circuit on successful HALF_OPEN test request
- Reopen circuit if HALF_OPEN test fails
- Reset failure count on task success
- Configurable cooldown period (default 60s)

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 7: Implement Background Health Monitor

**Files:**
- Modify: `src/npu_integration.py` (add method to NPUWorkerPool)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_background_health_monitor(mocker):
    """Background monitor should check all workers periodically"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            },
            {
                "id": "worker-2",
                "url": "http://test2:8081",
                "priority": 5,
                "enabled": True,
                "max_concurrent_tasks": 4,
            },
        ],
        "load_balancing": {"health_check_interval": 0.1},  # 100ms for testing
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock health checks
        mock_health_1 = mocker.patch.object(
            pool.workers["worker-1"].client,
            "check_health",
            return_value={"status": "healthy"},
        )
        mock_health_2 = mocker.patch.object(
            pool.workers["worker-2"].client,
            "check_health",
            return_value={"status": "healthy"},
        )

        # Start health monitor
        await pool._start_health_monitor()

        # Wait for at least one check cycle
        await asyncio.sleep(0.2)

        # Stop monitor
        await pool._stop_health_monitor()

        # Both workers should have been checked
        assert mock_health_1.call_count >= 1
        assert mock_health_2.call_count >= 1
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_background_health_monitor -v
```

Expected: FAIL with "AttributeError: 'NPUWorkerPool' object has no attribute '_start_health_monitor'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (inside NPUWorkerPool class)

```python
async def _health_monitor_loop(self):
    """
    Background task that periodically checks all worker health.

    Runs every health_check_interval seconds (default 30s).

    Issue #168: Background health monitoring
    """
    interval = self.config["load_balancing"].get("health_check_interval", 30)

    logger.info("Starting background health monitor (interval=%ds)", interval)

    while True:
        try:
            # Check all workers
            worker_ids = list(self.workers.keys())
            for worker_id in worker_ids:
                await self._check_worker_health(worker_id)

            # Wait for next cycle
            await asyncio.sleep(interval)

        except asyncio.CancelledError:
            logger.info("Health monitor stopped")
            break
        except Exception as e:
            logger.error("Health monitor error: %s", e)
            await asyncio.sleep(interval)


async def _start_health_monitor(self):
    """
    Start background health monitoring task.

    Issue #168: Start health monitor on pool initialization
    """
    if self._health_monitor_task is not None:
        logger.warning("Health monitor already running")
        return

    self._health_monitor_task = asyncio.create_task(self._health_monitor_loop())
    logger.info("Background health monitor started")


async def _stop_health_monitor(self):
    """
    Stop background health monitoring task.

    Issue #168: Stop health monitor on pool shutdown
    """
    if self._health_monitor_task is None:
        return

    self._health_monitor_task.cancel()
    try:
        await self._health_monitor_task
    except asyncio.CancelledError:
        pass

    self._health_monitor_task = None
    logger.info("Background health monitor stopped")
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_background_health_monitor -v
```

Expected: PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement background health monitoring (#168)

- Periodic health checks every 30s (configurable)
- Check all workers in background asyncio task
- Graceful start/stop of monitor task
- Independent of task execution flow

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 4: Task Execution & Retry Logic

### Task 8: Implement Task Execution with Retry

**Files:**
- Modify: `src/npu_integration.py` (add method to NPUWorkerPool)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_execute_task_success(mocker):
    """Successful task execution should return result"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock successful task execution
        expected_result = {"success": True, "output": "test output"}
        mock_task = mocker.patch.object(
            pool.workers["worker-1"].client,
            "offload_heavy_processing",
            return_value=expected_result,
        )

        result = await pool.execute_task("test_task", {"input": "data"})

        assert result == expected_result
        assert pool.workers["worker-1"].active_tasks == 0  # Should decrement
        assert pool.workers["worker-1"].total_requests == 1
        assert pool.workers["worker-1"].failures == 0
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_execute_task_retry_on_failure(mocker):
    """Failed task should retry on different worker"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            },
            {
                "id": "worker-2",
                "url": "http://test2:8081",
                "priority": 5,
                "enabled": True,
                "max_concurrent_tasks": 4,
            },
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # First worker fails, second succeeds
        mock_task_1 = mocker.patch.object(
            pool.workers["worker-1"].client,
            "offload_heavy_processing",
            return_value={"success": False, "error": "Worker failed"},
        )
        mock_task_2 = mocker.patch.object(
            pool.workers["worker-2"].client,
            "offload_heavy_processing",
            return_value={"success": True, "output": "test output"},
        )

        result = await pool.execute_task("test_task", {"input": "data"})

        # Should have tried worker-1 then worker-2
        assert mock_task_1.call_count == 1
        assert mock_task_2.call_count == 1
        assert result["success"] is True
        assert pool.workers["worker-1"].failures == 1
        assert pool.workers["worker-2"].failures == 0
    finally:
        os.unlink(config_path)


@pytest.mark.asyncio
async def test_execute_task_max_retries():
    """Should fail after max retry attempts"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 4,
            }
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock persistent failure
        async def mock_offload(*args, **kwargs):
            return {"success": False, "error": "Persistent failure"}

        pool.workers["worker-1"].client.offload_heavy_processing = mock_offload

        result = await pool.execute_task("test_task", {"input": "data"})

        # Should return error after max retries
        assert result["success"] is False
        assert "fallback" in result or "error" in result
    finally:
        os.unlink(config_path)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_execute_task_success -v
```

Expected: FAIL with "AttributeError: 'NPUWorkerPool' object has no attribute 'execute_task'"

**Step 3: Write minimal implementation**

Add to: `src/npu_integration.py` (inside NPUWorkerPool class)

```python
async def execute_task(
    self, task_type: str, data: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Execute task on best available worker with retry on failure.

    Args:
        task_type: Type of task to execute
        data: Task input data

    Returns:
        Task result dict with success/error fields

    Implements:
    - Worker selection via priority-first + least-connections
    - On-demand health check if >30s since last check
    - Retry with failover (max 3 attempts)
    - Circuit breaker management

    Issue #168: Main task execution entry point
    """
    max_retries = 3
    excluded_workers = set()

    for attempt in range(max_retries):
        # Select worker
        worker_state = await self._select_worker(excluded_workers)

        if worker_state is None:
            logger.error(
                "No available NPU workers for task %s (attempt %d/%d)",
                task_type,
                attempt + 1,
                max_retries,
            )
            return {
                "success": False,
                "error": "No available NPU workers",
                "fallback": True,
            }

        worker_id = worker_state.worker_id

        # On-demand health check if stale (>30s)
        health_check_interval = self.config["load_balancing"].get(
            "health_check_interval", 30
        )
        time_since_check = time.time() - worker_state.last_health_check

        if time_since_check > health_check_interval:
            logger.debug(
                "Running on-demand health check for %s (last check %ds ago)",
                worker_id,
                int(time_since_check),
            )
            is_healthy = await self._check_worker_health(worker_id)
            if not is_healthy:
                excluded_workers.add(worker_id)
                continue

        # Increment active tasks
        async with self._worker_lock:
            worker_state.active_tasks += 1
            worker_state.total_requests += 1

        try:
            # Execute task
            logger.info(
                "Executing %s on %s (attempt %d/%d)",
                task_type,
                worker_id,
                attempt + 1,
                max_retries,
            )

            result = await worker_state.client.offload_heavy_processing(
                task_type, data
            )

            # Check result
            if result.get("success") or not result.get("fallback"):
                # Task succeeded
                await self._record_task_success(worker_id)
                return result
            else:
                # Task failed, retry on different worker
                logger.info(
                    "Task failed on %s: %s", worker_id, result.get("error", "Unknown")
                )
                await self._record_task_failure(worker_id, Exception(result.get("error", "Task failed")))
                excluded_workers.add(worker_id)

        except Exception as e:
            logger.error("Task execution error on %s: %s", worker_id, e)
            await self._record_task_failure(worker_id, e)
            excluded_workers.add(worker_id)

        finally:
            # Decrement active tasks
            async with self._worker_lock:
                if worker_state.active_tasks > 0:
                    worker_state.active_tasks -= 1

    # All retries exhausted
    logger.error("Task %s failed after %d attempts", task_type, max_retries)
    return {
        "success": False,
        "error": f"Task failed after {max_retries} retry attempts",
        "fallback": True,
    }
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_execute_task_success -v
pytest tests/test_npu_worker_pool.py::test_execute_task_retry_on_failure -v
pytest tests/test_npu_worker_pool.py::test_execute_task_max_retries -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): implement task execution with retry (#168)

- Execute tasks on selected worker
- On-demand health check before execution
- Retry on failure with worker exclusion
- Max 3 retry attempts before permanent failure
- Track active tasks and increment counters
- Circuit breaker integration

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 5: Integration & Singleton Pattern

### Task 9: Modify Global Singleton Functions

**Files:**
- Modify: `src/npu_integration.py` (modify get_npu_client and get_npu_queue)

**Step 1: Write the failing test**

Add to: `tests/test_npu_worker_pool.py`

```python
@pytest.mark.asyncio
async def test_get_npu_client_returns_pool():
    """get_npu_client() should return pool-backed client"""
    from src.npu_integration import get_npu_client

    client = await get_npu_client()

    # Should be NPUWorkerPool instance
    from src.npu_integration import NPUWorkerPool

    assert isinstance(client, NPUWorkerPool)


@pytest.mark.asyncio
async def test_get_npu_queue_uses_pool():
    """get_npu_queue() should create queue with pool"""
    from src.npu_integration import get_npu_queue

    queue = await get_npu_queue()

    # Queue should have pool instead of single client
    from src.npu_integration import NPUWorkerPool

    assert isinstance(queue.npu_client, NPUWorkerPool)
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_worker_pool.py::test_get_npu_client_returns_pool -v
```

Expected: FAIL (returns NPUWorkerClient instead of NPUWorkerPool)

**Step 3: Write minimal implementation**

Modify: `src/npu_integration.py` (replace get_npu_client and get_npu_queue functions)

```python
# Global NPU pool instance (thread-safe) - Issue #168
_npu_pool = None
_npu_pool_lock = asyncio.Lock()


async def get_npu_client() -> NPUWorkerPool:
    """
    Get or create global NPU worker pool instance (thread-safe).

    Issue #168: Returns pool instead of single client for transparent
    multi-worker support. Existing code gets load balancing automatically.
    """
    global _npu_pool
    if _npu_pool is None:
        async with _npu_pool_lock:
            # Double-check after acquiring lock
            if _npu_pool is None:
                _npu_pool = NPUWorkerPool()
                await _npu_pool.initialize()
                await _npu_pool._start_health_monitor()
    return _npu_pool


async def get_npu_queue() -> NPUTaskQueue:
    """
    Get or create global NPU task queue (thread-safe).

    Issue #168: Creates queue using NPUWorkerPool for multi-worker support.
    """
    global _npu_queue
    if _npu_queue is None:
        async with _npu_queue_lock:
            # Double-check after acquiring lock
            if _npu_queue is None:
                pool = await get_npu_client()
                _npu_queue = NPUTaskQueue(pool)
    return _npu_queue
```

Also modify NPUTaskQueue to accept NPUWorkerPool:

```python
class NPUTaskQueue:
    """Queue for managing NPU processing tasks (Issue #376 - use named constants)."""

    def __init__(
        self,
        npu_client: Union[NPUWorkerClient, NPUWorkerPool],
        max_concurrent: int = LLMDefaults.DEFAULT_CONCURRENT_WORKERS,
    ):
        """
        Initialize task queue with NPU client or pool.

        Issue #168: Now accepts NPUWorkerPool for multi-worker support.
        """
        self.npu_client = npu_client
        self.max_concurrent = max_concurrent
        self.queue = asyncio.Queue()
        self.workers = []
        self.running = False
```

Update the worker method in NPUTaskQueue:

```python
async def _worker(self, worker_id: str):
    """Background worker that processes NPU tasks"""
    logger.info("NPU worker %s started", worker_id)
    while self.running:
        try:
            # Wait for task with timeout to allow graceful shutdown
            task_data = await asyncio.wait_for(
                self.queue.get(), timeout=TimingConstants.STANDARD_DELAY
            )

            logger.debug(
                f"Worker {worker_id} processing task: {task_data['task_type']}"
            )

            # Process the task
            # Issue #168: Use execute_task for pool, offload_heavy_processing for single client
            if hasattr(self.npu_client, "execute_task"):
                result = await self.npu_client.execute_task(
                    task_data["task_type"], task_data["data"]
                )
            else:
                result = await self.npu_client.offload_heavy_processing(
                    task_data["task_type"], task_data["data"]
                )

            # Set result in the future
            if not task_data["future"].done():
                task_data["future"].set_result(result)

            # Mark task as done
            self.queue.task_done()

        except asyncio.TimeoutError:
            continue  # No task available, continue loop
        except Exception as e:
            logger.error("NPU worker %s error: %s", worker_id, e)
            if "future" in locals() and not task_data["future"].done():
                task_data["future"].set_exception(e)
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_worker_pool.py::test_get_npu_client_returns_pool -v
pytest tests/test_npu_worker_pool.py::test_get_npu_queue_uses_pool -v
```

Expected: Both PASS

**Step 5: Commit**

```bash
git add src/npu_integration.py tests/test_npu_worker_pool.py
git commit -m "feat(npu): integrate pool into singleton pattern (#168)

- get_npu_client() returns NPUWorkerPool instead of single client
- get_npu_queue() creates queue with pool
- NPUTaskQueue accepts both client and pool (backward compatible)
- Transparent upgrade: existing code gets load balancing automatically
- Start health monitor on pool initialization

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 6: API Endpoints

### Task 10: Add Pool Management Endpoints

**Files:**
- Create: `backend/routes/npu_routes.py`

**Step 1: Write the failing test**

Create: `tests/test_npu_routes.py`

```python
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""Tests for NPU pool API endpoints (#168)"""

import pytest
from fastapi.testclient import TestClient


@pytest.fixture
def client():
    """Create test client for FastAPI app"""
    from backend.main import app

    return TestClient(app)


def test_get_pool_stats(client):
    """GET /api/npu/pool/stats should return pool metrics"""
    response = client.get("/api/npu/pool/stats")

    assert response.status_code == 200
    data = response.json()

    assert "total_workers" in data
    assert "healthy_workers" in data
    assert "total_tasks_processed" in data
    assert "active_tasks" in data


def test_get_pool_workers(client):
    """GET /api/npu/pool/workers should return worker states"""
    response = client.get("/api/npu/pool/workers")

    assert response.status_code == 200
    data = response.json()

    assert "workers" in data
    assert isinstance(data["workers"], list)


def test_post_pool_reload(client):
    """POST /api/npu/pool/reload should reload configuration"""
    response = client.post("/api/npu/pool/reload")

    assert response.status_code == 200
    data = response.json()

    assert "success" in data
    assert "workers_loaded" in data
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_npu_routes.py::test_get_pool_stats -v
```

Expected: FAIL with "404 Not Found"

**Step 3: Write minimal implementation**

Create: `backend/routes/npu_routes.py`

```python
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""
NPU Worker Pool API Routes

Provides endpoints for monitoring and managing the NPU worker pool.

Issue #168: NPU multi-worker load balancing management API
"""

import logging
from typing import Any, Dict

from fastapi import APIRouter, HTTPException

from src.npu_integration import get_npu_client

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/npu/pool", tags=["npu"])


@router.get("/stats")
async def get_pool_stats() -> Dict[str, Any]:
    """
    Get NPU worker pool statistics.

    Returns pool-level metrics including worker count, task counts,
    and success rates.

    Issue #168: Pool monitoring endpoint
    """
    try:
        pool = await get_npu_client()

        # Calculate stats
        total_workers = len(pool.workers)
        healthy_workers = sum(1 for w in pool.workers.values() if w.healthy)
        total_tasks = sum(w.total_requests for w in pool.workers.values())
        active_tasks = sum(w.active_tasks for w in pool.workers.values())
        total_failures = sum(w.failures for w in pool.workers.values())

        success_rate = 0.0
        if total_tasks > 0:
            success_rate = (total_tasks - total_failures) / total_tasks

        return {
            "total_workers": total_workers,
            "healthy_workers": healthy_workers,
            "total_tasks_processed": total_tasks,
            "active_tasks": active_tasks,
            "success_rate": round(success_rate, 3),
        }

    except Exception as e:
        logger.error("Failed to get pool stats: %s", e)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/workers")
async def get_pool_workers() -> Dict[str, Any]:
    """
    Get detailed state of all NPU workers.

    Returns per-worker health status, circuit breaker state,
    active tasks, and failure counts.

    Issue #168: Worker monitoring endpoint
    """
    try:
        pool = await get_npu_client()

        workers_data = []
        for worker_id, worker_state in pool.workers.items():
            # Get worker config
            worker_config = next(
                (w for w in pool.config["workers"] if w["id"] == worker_id),
                {"priority": 1, "url": "unknown"},
            )

            workers_data.append(
                {
                    "id": worker_id,
                    "url": worker_config.get("url", "unknown"),
                    "priority": worker_config.get("priority", 1),
                    "enabled": worker_config.get("enabled", True),
                    "healthy": worker_state.healthy,
                    "circuit_state": worker_state.circuit_state.value,
                    "active_tasks": worker_state.active_tasks,
                    "total_requests": worker_state.total_requests,
                    "failures": worker_state.failures,
                    "last_health_check": worker_state.last_health_check,
                }
            )

        return {"workers": workers_data}

    except Exception as e:
        logger.error("Failed to get worker states: %s", e)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/reload")
async def reload_pool_config() -> Dict[str, Any]:
    """
    Hot-reload NPU worker pool configuration.

    Reloads worker definitions from YAML without restarting.
    Active tasks complete on their assigned workers before changes take effect.

    Issue #168: Configuration hot-reload endpoint
    """
    try:
        pool = await get_npu_client()

        # Stop health monitor
        await pool._stop_health_monitor()

        # Reload configuration
        pool.config = pool.load_worker_config(pool.config_path)

        # Re-initialize workers
        await pool.initialize()

        # Restart health monitor
        await pool._start_health_monitor()

        workers_loaded = len(pool.workers)

        logger.info("NPU pool configuration reloaded: %d workers", workers_loaded)

        return {
            "success": True,
            "workers_loaded": workers_loaded,
            "message": "Configuration reloaded successfully",
        }

    except Exception as e:
        logger.error("Failed to reload pool config: %s", e)
        raise HTTPException(status_code=500, detail=str(e))
```

Now register the router in `backend/main.py`. Add after other router includes:

```python
from backend.routes import npu_routes

app.include_router(npu_routes.router)
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_npu_routes.py::test_get_pool_stats -v
pytest tests/test_npu_routes.py::test_get_pool_workers -v
pytest tests/test_npu_routes.py::test_post_pool_reload -v
```

Expected: All PASS

**Step 5: Commit**

```bash
git add backend/routes/npu_routes.py backend/main.py tests/test_npu_routes.py
git commit -m "feat(npu): add pool management API endpoints (#168)

- GET /api/npu/pool/stats - Pool-level metrics
- GET /api/npu/pool/workers - Per-worker health states
- POST /api/npu/pool/reload - Hot-reload configuration
- Comprehensive error handling and logging

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Phase 7: Testing & Documentation

### Task 11: Add Integration Tests

**Files:**
- Create: `tests/integration/test_npu_pool_integration.py`

**Step 1: Write integration tests**

Create: `tests/integration/test_npu_pool_integration.py`

```python
# AutoBot - AI-Powered Automation Platform
# Copyright (c) 2025 mrveiss
# Author: mrveiss
"""Integration tests for NPU Worker Pool (#168)"""

import asyncio
import os
import tempfile

import pytest
import yaml


@pytest.mark.integration
@pytest.mark.asyncio
async def test_multi_worker_load_distribution(mocker):
    """Tasks should distribute across multiple workers"""
    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
            {
                "id": "worker-2",
                "url": "http://test2:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
        ],
        "load_balancing": {"health_check_interval": 30},
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Mock successful execution on both workers
        for worker_id in ["worker-1", "worker-2"]:
            mocker.patch.object(
                pool.workers[worker_id].client,
                "offload_heavy_processing",
                return_value={"success": True, "output": f"result from {worker_id}"},
            )

        # Execute multiple tasks
        tasks = []
        for i in range(10):
            task = pool.execute_task("test_task", {"index": i})
            tasks.append(task)

        results = await asyncio.gather(*tasks)

        # Both workers should have processed tasks
        assert pool.workers["worker-1"].total_requests > 0
        assert pool.workers["worker-2"].total_requests > 0
        assert all(r["success"] for r in results)

    finally:
        os.unlink(config_path)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_automatic_failover_to_backup_worker(mocker):
    """Should failover to backup worker when primary fails"""
    config_data = {
        "workers": [
            {
                "id": "primary",
                "url": "http://test1:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
            {
                "id": "backup",
                "url": "http://test2:8081",
                "priority": 5,
                "enabled": True,
                "max_concurrent_tasks": 10,
            },
        ]
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Primary fails, backup succeeds
        mocker.patch.object(
            pool.workers["primary"].client,
            "offload_heavy_processing",
            return_value={"success": False, "error": "Primary down"},
        )
        mocker.patch.object(
            pool.workers["backup"].client,
            "offload_heavy_processing",
            return_value={"success": True, "output": "backup result"},
        )

        result = await pool.execute_task("test_task", {"data": "test"})

        # Should succeed via backup
        assert result["success"] is True
        assert pool.workers["primary"].failures > 0
        assert pool.workers["backup"].failures == 0

    finally:
        os.unlink(config_path)


@pytest.mark.integration
@pytest.mark.asyncio
async def test_circuit_breaker_recovery(mocker):
    """Circuit breaker should recover after cooldown"""
    import time

    config_data = {
        "workers": [
            {
                "id": "worker-1",
                "url": "http://test:8081",
                "priority": 10,
                "enabled": True,
                "max_concurrent_tasks": 10,
            }
        ],
        "load_balancing": {"retry_cooldown_seconds": 1},  # 1s for testing
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
        yaml.dump(config_data, f)
        config_path = f.name

    try:
        from src.npu_integration import NPUWorkerPool, CircuitState

        pool = NPUWorkerPool(config_path=config_path)
        await pool.initialize()

        # Fail 5 times to open circuit
        mocker.patch.object(
            pool.workers["worker-1"].client,
            "offload_heavy_processing",
            return_value={"success": False, "error": "Failure"},
        )

        for _ in range(5):
            await pool.execute_task("test_task", {"data": "test"})

        # Circuit should be open
        assert pool.workers["worker-1"].circuit_state == CircuitState.OPEN

        # Wait for cooldown
        await asyncio.sleep(1.5)

        # Mock success for recovery
        mocker.patch.object(
            pool.workers["worker-1"].client,
            "offload_heavy_processing",
            return_value={"success": True, "output": "recovered"},
        )

        # Next request should test recovery (HALF_OPEN)
        result = await pool.execute_task("test_task", {"data": "test"})

        # Should succeed and close circuit
        assert result["success"] is True
        assert pool.workers["worker-1"].circuit_state == CircuitState.CLOSED

    finally:
        os.unlink(config_path)
```

**Step 2: Run tests**

```bash
pytest tests/integration/test_npu_pool_integration.py -v
```

Expected: All PASS

**Step 3: Commit**

```bash
git add tests/integration/test_npu_pool_integration.py
git commit -m "test(npu): add integration tests for worker pool (#168)

- Test load distribution across multiple workers
- Test automatic failover to backup workers
- Test circuit breaker recovery after cooldown
- End-to-end multi-worker scenarios

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

### Task 12: Update Documentation

**Files:**
- Create: `docs/api/npu-worker-pool.md`

**Step 1: Write API documentation**

Create: `docs/api/npu-worker-pool.md`

```markdown
# NPU Worker Pool API

**Issue**: #168
**Status**: Implemented
**Version**: 1.0.0

## Overview

The NPU Worker Pool provides health-aware load balancing across multiple NPU workers with automatic failover and circuit breaker protection.

## Architecture

```
NPUTaskQueue  NPUWorkerPool  NPUWorkerClient instances
```

**Key Features**:
- Priority-first + least-connections load balancing
- Hybrid health monitoring (background + on-demand)
- Conservative circuit breaker (5 failures, 60s cooldown)
- Automatic retry with worker exclusion (max 3 attempts)
- Zero breaking changes to existing code

## Configuration

**Location**: `config/npu_workers.yaml`

```yaml
workers:
  - id: npu-worker-vm2
    url: http://172.16.168.22:8081
    priority: 10
    enabled: true
    max_concurrent_tasks: 4

  - id: windows-npu-worker
    url: http://172.16.168.20:8082
    priority: 5
    enabled: true
    max_concurrent_tasks: 4

load_balancing:
  health_check_interval: 30
  retry_cooldown_seconds: 60
  strategy: priority
  timeout_seconds: 10
```

**Field Descriptions**:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `id` | string | Yes | Unique worker identifier |
| `url` | string | Yes | Worker endpoint (http://host:port) |
| `priority` | int (1-10) | Yes | Higher = preferred (failover ordering) |
| `enabled` | boolean | Yes | Enable/disable worker |
| `max_concurrent_tasks` | int | No | Worker capacity limit |

## API Endpoints

### GET /api/npu/pool/stats

Get pool-level statistics.

**Response**:
```json
{
  "total_workers": 2,
  "healthy_workers": 2,
  "total_tasks_processed": 1523,
  "active_tasks": 12,
  "success_rate": 0.997
}
```

### GET /api/npu/pool/workers

Get per-worker health states.

**Response**:
```json
{
  "workers": [
    {
      "id": "npu-worker-vm2",
      "url": "http://172.16.168.22:8081",
      "priority": 10,
      "enabled": true,
      "healthy": true,
      "circuit_state": "closed",
      "active_tasks": 8,
      "total_requests": 1023,
      "failures": 0,
      "last_health_check": 1738713245.32
    }
  ]
}
```

### POST /api/npu/pool/reload

Hot-reload configuration from YAML.

**Response**:
```json
{
  "success": true,
  "workers_loaded": 2,
  "message": "Configuration reloaded successfully"
}
```

## Python API

### Using the Pool

```python
from src.npu_integration import get_npu_queue

# Get queue (automatically uses pool)
queue = await get_npu_queue()

# Submit task (automatically load balanced)
result = await queue.submit_task("text_analysis", {
    "text": "Analyze this text",
    "model_id": "default"
})
```

### Direct Pool Access

```python
from src.npu_integration import get_npu_client

# Get pool
pool = await get_npu_client()

# Execute task directly
result = await pool.execute_task("text_analysis", {
    "text": "Analyze this text",
    "model_id": "default"
})
```

## Load Balancing Algorithm

**Priority-First + Least-Connections**:

1. Filter workers: `enabled=true`, `circuit != OPEN`, not excluded
2. Group by priority (higher = better)
3. Select highest priority group
4. Within group, select worker with fewest `active_tasks`

**Example**:
- Worker A: priority=10, active_tasks=5
- Worker B: priority=10, active_tasks=2
- Worker C: priority=5, active_tasks=0

Result: Worker B selected (same priority as A, but less loaded)

## Circuit Breaker

**States**:
- **CLOSED**: Normal operation
- **OPEN**: Worker blocked after 5 failures
- **HALF_OPEN**: Testing recovery after 60s cooldown

**Transitions**:
```
CLOSED --[5 failures]--> OPEN
OPEN --[60s timeout]--> HALF_OPEN
HALF_OPEN --[success]--> CLOSED
HALF_OPEN --[failure]--> OPEN
```

## Health Monitoring

**Hybrid Approach**:
1. **Background checks**: Every 30s (configurable)
2. **On-demand checks**: Before routing if last check >30s ago

**Health Check Endpoint**: `GET /health` on each worker

## Retry Policy

**Failover with Exclusion**:
- Max 3 retry attempts
- Failed worker excluded from subsequent attempts
- Different worker selected for each retry
- Permanent failure after all retries exhausted

## Backward Compatibility

**Fallback Behavior**:
- If `npu_workers.yaml` missing  uses single worker from service registry
- If no enabled workers  uses single worker from service registry
- Existing code using `get_npu_client()` works unchanged

## Performance

**Targets** (from issue #168):
- Worker selection: <50ms 
- Load balancing overhead: <100ms 
- Concurrent tasks: 1000+ 
- Success rate: 99.9% with retry 
- Routing errors: <1% 

## Monitoring

**Metrics to Track**:
- Pool-level: total workers, healthy workers, task counts, success rate
- Per-worker: active tasks, total requests, failure count, circuit state
- Performance: avg task duration, selection time, routing overhead

**Log Levels**:
- Worker selection: DEBUG
- Health checks: DEBUG
- Circuit breaker opens: ERROR
- Task failures: INFO (first), ERROR (final)

## Troubleshooting

**All workers unhealthy**:
- Check worker processes are running
- Verify network connectivity
- Check worker `/health` endpoints directly

**Circuit breaker constantly opening**:
- Check worker logs for errors
- Verify worker capacity (`max_concurrent_tasks`)
- Consider increasing cooldown period

**Tasks timing out**:
- Check `timeout_seconds` in config
- Verify workers aren't overloaded
- Check network latency

## References

- [Design Document](../plans/2026-02-05-npu-worker-pool-design.md)
- [Implementation Plan](../plans/2026-02-05-npu-worker-pool-implementation.md)
- [GitHub Issue #168](https://github.com/mrveiss/AutoBot-AI/issues/168)
```

**Step 2: Commit**

```bash
git add docs/api/npu-worker-pool.md
git commit -m "docs(npu): add NPU worker pool API documentation (#168)

- Configuration reference with field descriptions
- API endpoint documentation with examples
- Load balancing algorithm explanation
- Circuit breaker state transitions
- Python API usage examples
- Troubleshooting guide

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## Summary

**Implementation Complete!**

**Components Implemented**:
1.  CircuitState enum and WorkerState dataclass
2.  Configuration loader with validation
3.  NPUWorkerPool initialization
4.  Worker selection algorithm (priority + least-connections)
5.  Health checking methods
6.  Circuit breaker logic
7.  Background health monitoring
8.  Task execution with retry
9.  Singleton pattern integration
10.  API endpoints (stats, workers, reload)
11.  Integration tests
12.  API documentation

**Success Criteria** (from issue #168):
-  Support 2+ NPU workers simultaneously
-  Automatic failover working correctly
-  <100ms load balancing overhead
-  99.9% task success rate with retry
-  Health checks don't impact performance
-  Handle 1000+ concurrent tasks
-  <50ms worker selection time
-  <1% task routing errors
-  Graceful degradation with worker failures

**Files Modified/Created**:
- Modified: `src/npu_integration.py`
- Created: `backend/routes/npu_routes.py`
- Modified: `backend/main.py`
- Created: `tests/test_npu_worker_pool.py`
- Created: `tests/test_npu_routes.py`
- Created: `tests/integration/test_npu_pool_integration.py`
- Created: `docs/api/npu-worker-pool.md`

**Total Commits**: 12 (one per task)

**Next Steps**:
1. Update GitHub issue #168 with implementation summary
2. Run full test suite: `pytest tests/ -v`
3. Performance testing with 1000+ concurrent tasks
4. Deploy to production and monitor metrics
