# Unified Docker Compose for AutoBot
# This file consolidates all service configurations with environment-based overrides

name: autobot

services:
  # DNS Cache Service - Provides fast DNS resolution for all containers
  dns-cache:
    build:
      context: .
      dockerfile: docker/dns-cache/Dockerfile
    image: autobot-dns-cache:latest
    container_name: autobot-dns-cache
    restart: unless-stopped
    volumes:
      - dns_cache:/var/cache/autobot
      - ./logs:/shared/logs  # Share logs with host
    environment:
      - DNS_REFRESH_INTERVAL=15
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "dns-cache"
    healthcheck:
      test: ["CMD", "nslookup", "host.docker.internal", "127.0.0.1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_DNS_CACHE_IP:-172.16.168.10}
    ports:
      - "5353:53/udp"  # Alternative DNS port to avoid conflicts
  # Redis Stack - Core data store
  redis:
    image: redis/redis-stack:7.4.0-v1
    container_name: autobot-redis
    hostname: redis.autobot
    restart: unless-stopped
    volumes:
      - redis_data:/data
      - ./logs:/shared/logs  # Share logs with host
    environment:
      - REDIS_ARGS=--appendonly yes --save 60 1
      - REDISINSIGHT_PORT=${REDISINSIGHT_PORT:-8002}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "redis"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_REDIS_HOST:-172.16.168.23}
        aliases:
          - redis.autobot
          - redis
    ports:
      - "${REDIS_PORT:-6379}:6379"
      - "${REDISINSIGHT_PORT:-8002}:8001"


  # Backend API - Runs on HOST for system access (desktop, network, files)
  # Started by unified script, not Docker

  # Browser Service with Playwright automation
  browser-service:
    build: 
      context: ./docker/browser
    image: autobot-browser:latest
    container_name: autobot-browser
    restart: unless-stopped
    environment:
      - BROWSER_PORT=3000
      - PLAYWRIGHT_BROWSERS_PATH=/root/.cache/ms-playwright
    volumes:
      - browser_data:/app/data
      - ./logs:/shared/logs  # Share logs with host
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "browser-service"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      start_period: 20s
      retries: 3
    ports:
      - "${BROWSER_PORT:-3000}:3000"    # Playwright API
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_BROWSER_SERVICE_HOST:-172.16.168.25}

  # FIXED: Frontend with proper multi-mode volume strategy
  frontend:
    build:
      context: ./autobot-vue
      dockerfile: ../docker/frontend/Dockerfile
      args:
        - NODE_VERSION=${NODE_VERSION:-20}
    image: autobot-frontend:latest
    container_name: autobot-frontend
    restart: unless-stopped
    depends_on:
      - redis
    # Revert to bridge networking with proper host resolution
    extra_hosts:
      - "host.docker.internal:host-gateway"
      - "backend.autobot:host-gateway"
    volumes:
      # CRITICAL FIX: Use conditional volume mounting to prevent race conditions
      # Development Mode: Mount source AND preserve node_modules via named volume
      - ./autobot-vue:/app:delegated
      # SEPARATED: Use init volume strategy instead of conflicting overlay
      - frontend_node_modules:/app/node_modules:consistent
      - ./docker/dns-sidecar:/opt/autobot-dns:ro
      - ./logs:/shared/logs  # Share logs with host
    environment:
      - NODE_ENV=${NODE_ENV:-development}
      - VITE_BACKEND_HOST=${AUTOBOT_BACKEND_HOST:-172.16.168.20}
      - VITE_BACKEND_PORT=${AUTOBOT_BACKEND_PORT:-8001}
      - DOCKER_CONTAINER=true  # Signal that we're running in Docker
      # ARCHITECTURE FIX: Enable comprehensive mode detection
      - AUTOBOT_EXECUTION_MODE=${AUTOBOT_EXECUTION_MODE:-development}
      - DEPENDENCY_CHECK_MODE=comprehensive
      - MOUNT_STRATEGY=named_volume_overlay
      # Use host.docker.internal for Docker container to WSL communication
      # Logging configuration
      - DEBUG_PROXY=${DEBUG_PROXY:-false}   # Only log proxy requests when explicitly enabled
      - LOG_LEVEL=${FRONTEND_LOG_LEVEL:-info}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "frontend"
    # IMPROVED: Conditional command based on execution mode
    command: >
      sh -c "
        echo 'Starting in mode: ${AUTOBOT_EXECUTION_MODE:-development}';
        if [ '${NODE_ENV:-development}' = 'production' ]; then
          npm run preview -- --host 0.0.0.0 --port 5173;
        else
          npm run dev -- --host 0.0.0.0 --port 5173;
        fi
      "
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:5173/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_FRONTEND_HOST:-172.16.168.21}
        aliases:
          - frontend.autobot
          - frontend
    ports:
      - "${FRONTEND_PORT:-5173}:5173"

  # AI Stack Service
  ai-stack:
    build:
      context: .
      dockerfile: docker/Dockerfile.ai-stack
    image: autobot-ai-stack:latest
    container_name: autobot-ai-stack
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./src:/app/src
      - ./config:/app/config
      - ./data:/app/data
      - ./prompts:/app/prompts
      - models:/app/models
      - ai_logs:/app/logs
      - ./logs:/shared/logs  # Share logs with host
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "ai-stack"
    environment:
      - PYTHONPATH=/app
      - AI_SERVER_HOST=0.0.0.0
      - AI_SERVER_PORT=${AI_SERVER_PORT:-8080}
      - AI_SERVER_WORKERS=1
      - REDIS_HOST=${AUTOBOT_REDIS_HOST:-172.16.168.23}
      - REDIS_PORT=${AUTOBOT_REDIS_PORT:-6379}
      - OLLAMA_HOST=${AUTOBOT_BACKEND_HOST:-172.16.168.20}:${AUTOBOT_OLLAMA_PORT:-11434}
      - CHROMA_DB_PATH=/app/data/chroma_db
      - DISABLE_EXTERNAL_APIS=${DISABLE_EXTERNAL_APIS:-true}
      - TF_USE_LEGACY_KERAS=1
      - KERAS_BACKEND=tensorflow
    command: python -m uvicorn ai_api_server:app --host 0.0.0.0 --port ${AI_SERVER_PORT:-8080}
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_AI_STACK_HOST:-172.16.168.24}
    ports:
      - "${AI_SERVER_PORT:-8080}:${AI_SERVER_PORT:-8080}"

  # NPU Worker Service  
  npu-worker:
    build:
      context: .
      dockerfile: docker/npu-worker/Dockerfile.npu-worker-simple-worker
    image: autobot-npu-worker:latest
    container_name: autobot-npu-worker
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - .:/app/autobot_codebase:ro  # Mount entire AutoBot codebase for analysis
      - ./config:/app/config
      - models:/app/models
      - npu_logs:/app/logs
      - ./logs:/shared/logs  # Share logs with host
      # Hardware access for NPU
      - /dev:/dev:ro
      - /lib/firmware:/lib/firmware:ro
      - /usr/local/lib:/usr/local/lib:ro
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "npu-worker"
    environment:
      - PYTHONPATH=/app
      - NPU_WORKER_HOST=0.0.0.0
      - NPU_WORKER_PORT=${NPU_WORKER_PORT:-8081}
      - NPU_WORKER_WORKERS=1
      - REDIS_HOST=${AUTOBOT_REDIS_HOST:-172.16.168.23}
      - REDIS_PORT=${AUTOBOT_REDIS_PORT:-6379}
      - NPU_LOG_LEVEL=${NPU_LOG_LEVEL:-INFO}
      - OPENVINO_DEVICE=${OPENVINO_DEVICE:-AUTO}
      - OV_LOG_LEVEL=${OV_LOG_LEVEL:-ERROR}
      - LD_LIBRARY_PATH=/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib/python3.10
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - TF_USE_LEGACY_KERAS=1
      - KERAS_BACKEND=tensorflow
    privileged: true
    # GPU support (if available)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      autobot:
        ipv4_address: ${AUTOBOT_NPU_WORKER_HOST:-172.16.168.22}
    ports:
      - "${NPU_WORKER_PORT:-8081}:${NPU_WORKER_PORT:-8081}"

volumes:
  dns_cache:
    name: autobot_dns_cache
  redis_data:
    name: autobot_redis_data
  models:
    name: autobot_models
  browser_data:
    name: autobot_browser_data
  # CRITICAL: Standard named volume for node_modules persistence
  frontend_node_modules:
    name: autobot_frontend_node_modules
  ai_logs:
    name: autobot_ai_logs
  npu_logs:
    name: autobot_npu_logs

networks:
  autobot:
    name: autobot-network
    driver: bridge
    ipam:
      config:
        - subnet: ${DOCKER_SUBNET:-172.16.168.0/24}
          gateway: ${DOCKER_GATEWAY:-172.16.168.1}