# AutoBot Environment Configuration
# Copy this file to .env and adjust values for your environment

# IP Addressing Scheme
# 127.0.0.1 - Reserved for internal processes
# 127.0.0.2 - Host machine (Ollama, LM Studio, external services)
# 127.0.0.3 - WSL/Primary host (AutoBot backend, frontend)
# 127.0.0.4 - Playwright VNC container
# 127.0.0.5 - NPU Worker container
# 127.0.0.6 - AI Stack container
# 127.0.0.7 - Redis container
# 127.0.0.8 - Log Viewer (Seq) container

# Service Host IP Addresses
# Ollama typically runs on localhost (default configuration)
AUTOBOT_OLLAMA_HOST=127.0.0.1
AUTOBOT_LM_STUDIO_HOST=127.0.0.2
AUTOBOT_BACKEND_HOST=127.0.0.3
AUTOBOT_FRONTEND_HOST=127.0.0.3
AUTOBOT_PLAYWRIGHT_HOST=127.0.0.4
AUTOBOT_NPU_WORKER_HOST=127.0.0.5
AUTOBOT_AI_STACK_HOST=127.0.0.6
AUTOBOT_REDIS_HOST=127.0.0.7
AUTOBOT_LOG_VIEWER_HOST=127.0.0.8

# Service Ports
AUTOBOT_BACKEND_PORT=8001
AUTOBOT_FRONTEND_PORT=5173
AUTOBOT_OLLAMA_PORT=11434
AUTOBOT_LM_STUDIO_PORT=1234
AUTOBOT_REDIS_PORT=6379
AUTOBOT_PLAYWRIGHT_API_PORT=3000
AUTOBOT_PLAYWRIGHT_VNC_PORT=6080
AUTOBOT_NPU_WORKER_PORT=8081
AUTOBOT_AI_STACK_PORT=8080
AUTOBOT_LOG_VIEWER_PORT=5341
AUTOBOT_FLUENTD_PORT=24224
AUTOBOT_CHROME_DEBUG_PORT=9222
AUTOBOT_VNC_DISPLAY_PORT=5900
AUTOBOT_VNC_CONTAINER_PORT=5901

# Protocols
AUTOBOT_HTTP_PROTOCOL=http
AUTOBOT_WS_PROTOCOL=ws
AUTOBOT_REDIS_PROTOCOL=redis

# Primary Services (Built from components above)
AUTOBOT_API_BASE_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_BACKEND_HOST}:${AUTOBOT_BACKEND_PORT}
AUTOBOT_FRONTEND_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_FRONTEND_HOST}:${AUTOBOT_FRONTEND_PORT}
AUTOBOT_REDIS_URL=${AUTOBOT_REDIS_PROTOCOL}://${AUTOBOT_REDIS_HOST}:${AUTOBOT_REDIS_PORT}
AUTOBOT_WS_BASE_URL=${AUTOBOT_WS_PROTOCOL}://${AUTOBOT_BACKEND_HOST}:${AUTOBOT_BACKEND_PORT}/ws

# AI Services (can run on any host)
AUTOBOT_OLLAMA_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_OLLAMA_HOST}:${AUTOBOT_OLLAMA_PORT}
AUTOBOT_LM_STUDIO_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_LM_STUDIO_HOST}:${AUTOBOT_LM_STUDIO_PORT}

# Container Services
AUTOBOT_PLAYWRIGHT_API_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_PLAYWRIGHT_HOST}:${AUTOBOT_PLAYWRIGHT_API_PORT}
AUTOBOT_PLAYWRIGHT_VNC_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_PLAYWRIGHT_HOST}:${AUTOBOT_PLAYWRIGHT_VNC_PORT}/vnc.html
AUTOBOT_NPU_WORKER_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_NPU_WORKER_HOST}:${AUTOBOT_NPU_WORKER_PORT}
AUTOBOT_AI_STACK_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_AI_STACK_HOST}:${AUTOBOT_AI_STACK_PORT}

# Logging and Monitoring
AUTOBOT_LOG_VIEWER_URL=${AUTOBOT_HTTP_PROTOCOL}://${AUTOBOT_LOG_VIEWER_HOST}:${AUTOBOT_LOG_VIEWER_PORT}
AUTOBOT_FLUENTD_ADDRESS=${AUTOBOT_LOG_VIEWER_HOST}:${AUTOBOT_FLUENTD_PORT}

# Development Settings
AUTOBOT_API_TIMEOUT=30000
AUTOBOT_VNC_PASSWORD=<GENERATE_SECURE_PASSWORD>

# Timeout Configuration (in seconds)
AUTOBOT_HEALTH_CHECK_TIMEOUT=3
AUTOBOT_API_REQUEST_TIMEOUT=10
AUTOBOT_LLM_REQUEST_TIMEOUT=30
AUTOBOT_WEBSOCKET_TIMEOUT=30
AUTOBOT_UPLOAD_TIMEOUT=300

# Deployment Mode
AUTOBOT_DEPLOYMENT_MODE=hybrid
AUTOBOT_DOMAIN=autobot.local

# ============================================================================
# DISTRIBUTED VM NETWORK CONFIGURATION
# ============================================================================
# For distributed deployments across multiple VMs
# These variables override the defaults in src/constants/network_constants.py
#
# Main Machine (WSL) - Hosts backend and Windows NPU worker
MAIN_MACHINE_IP=172.16.168.20
#
# Remote VMs
FRONTEND_VM_IP=172.16.168.21
NPU_WORKER_VM_IP=172.16.168.22
REDIS_VM_IP=172.16.168.23
AI_STACK_VM_IP=172.16.168.24
BROWSER_VM_IP=172.16.168.25
#
# Standard Service Ports (distributed mode)
# BACKEND_PORT=8001  (already defined above as AUTOBOT_BACKEND_PORT)
# REDIS_PORT=6379    (already defined above as AUTOBOT_REDIS_PORT)
NPU_WORKER_PORT=8081
NPU_WORKER_WINDOWS_PORT=8082
AI_STACK_PORT=8080
BROWSER_SERVICE_PORT=3000
#
# Service URLs (distributed mode)
# These are auto-constructed from IPs and ports by ServiceURLs in network_constants.py
# BACKEND_API=http://172.16.168.20:8001
# FRONTEND_VM=http://172.16.168.21:5173
# REDIS_VM=redis://172.16.168.23:6379
# NPU_WORKER_SERVICE=http://172.16.168.22:8081
# NPU_WORKER_WINDOWS_SERVICE=http://172.16.168.20:8082
# AI_STACK_SERVICE=http://172.16.168.24:8080
# BROWSER_SERVICE=http://172.16.168.25:3000

# ============================================================================
# PATH CONFIGURATION
# ============================================================================
# These paths are auto-detected by src/constants/path_constants.py
# Override only if you need custom paths
#
# PROJECT_ROOT - Auto-detected from script location
# USER_HOME - Auto-detected from Path.home()
#
# Core directories (relative to PROJECT_ROOT):
# - config/       Configuration files
# - data/         Application data
# - logs/         Log files
# - tests/        Test files
# - backend/      Backend API code
# - frontend/     Frontend Vue.js code
# - scripts/      Utility scripts
# - analysis/     Analysis outputs
# - reports/      Generated reports
# - mcp-tools/    MCP server tools
#
# Data subdirectories:
# - data/conversations/     Chat conversation storage
# - data/chat_history/      Chat history files
# - data/system_knowledge/  System knowledge base
# - data/checkpoints/       Long-running operation checkpoints
# - data/security/          Security-related data
#
# Common log files:
# - logs/backend.log        Backend application log
# - logs/frontend.log       Frontend application log
# - logs/redis.log          Redis service log
# - logs/audit/             Audit logs directory
#
# SSH Keys:
# - ~/.ssh/autobot_key      SSH key for VM authentication (4096-bit RSA)

# ===== LLM MODEL CONFIGURATION =====
# Central configuration for all LLM model references across AutoBot
# 
# CRITICAL: All code should use these environment variables instead of hardcoded model names
# - Prevents maintenance nightmare of updating 14+ files when models change
# - Allows easy switching between models based on performance/resource needs
# - Single source of truth for model configuration
#
# Available models (check with: ollama list):
#   llama3.2:1b (1.3GB)          - Fastest, lowest resource usage, good for simple tasks
#   mistral:7b-instruct (4.4GB)  - Balanced quality and performance
#   dolphin-llama3:8b (4.7GB)    - High quality responses
#   dolphin-mixtral:8x7b (26GB)  - Maximum quality (requires significant RAM)
#   openchat:7b (4.1GB)          - Optimized for conversation
#
# Default model used by all agents/services (unless specialized model specified below)
AUTOBOT_DEFAULT_LLM_MODEL=mistral:7b-instruct

# Specialized model assignments (optional - each defaults to AUTOBOT_DEFAULT_LLM_MODEL if not set)
# Uncomment and modify only if you need different models for specific functions
# AUTOBOT_ORCHESTRATOR_MODEL=mistral:7b-instruct  # Workflow coordination
# AUTOBOT_DEFAULT_AGENT_MODEL=llama3.2:1b        # General agent tasks
# AUTOBOT_RESEARCH_MODEL=dolphin-llama3:8b       # Web research and analysis
# AUTOBOT_ANALYSIS_MODEL=mistral:7b-instruct     # Code/data analysis
# AUTOBOT_PLANNING_MODEL=llama3.2:1b             # Task planning
# AUTOBOT_RAG_MODEL=llama3.2:1b                  # RAG/knowledge base queries

# Embedding model for vector operations (keep as nomic-embed-text unless specific needs)
AUTOBOT_EMBEDDING_MODEL=nomic-embed-text:latest
