# AlertManager Configuration for AutoBot
# Phase 3: Alert Migration (Issue #346)
# Enhanced with notification channels (Issue #69)
# Copyright (c) 2025 mrveiss
# Author: mrveiss

global:
  # Global configuration
  resolve_timeout: 5m  # Time to wait before considering alert resolved

  # SMTP Configuration (Issue #69) - Configure via environment variables
  # smtp_smarthost: 'smtp.gmail.com:587'
  # smtp_from: 'autobot-alerts@example.com'
  # smtp_auth_username: '${ALERT_EMAIL_USERNAME}'
  # smtp_auth_password: '${ALERT_EMAIL_PASSWORD}'
  # smtp_require_tls: true

  # Slack Configuration (Issue #69) - Configure via environment variables
  # slack_api_url: '${ALERT_SLACK_WEBHOOK_URL}'

route:
  # Root route - all alerts pass through here
  receiver: 'default'
  group_by: ['alertname', 'severity', 'component']
  group_wait: 10s        # Wait before sending first notification
  group_interval: 10s    # Wait before sending batch of grouped alerts
  repeat_interval: 4h    # Wait before re-sending same alert

  # Route hierarchy for different alert types
  routes:
    # Critical alerts - immediate notification (WebSocket + optional email/Slack)
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      group_interval: 5s
      repeat_interval: 30m
      continue: true  # Also send to default receiver

    # High severity alerts
    - match:
        severity: high
      receiver: 'high-alerts'
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Medium severity alerts
    - match:
        severity: medium
      receiver: 'medium-alerts'
      repeat_interval: 2h
      continue: true

    # Circuit breaker alerts (Issue #69) - fast notification
    - match:
        component: circuit_breaker
      receiver: 'circuit-breaker-alerts'
      group_wait: 5s
      repeat_interval: 15m
      continue: true

    # Redis alerts (Issue #69)
    - match:
        component: redis
      receiver: 'redis-alerts'
      group_wait: 10s
      repeat_interval: 30m
      continue: true

    # NPU alerts (Issue #69)
    - match:
        component: npu
      receiver: 'npu-alerts'
      group_wait: 10s
      repeat_interval: 30m
      continue: true

    # Security alerts (Issue #69) - immediate notification
    - match:
        component: security
      receiver: 'security-alerts'
      group_wait: 5s
      repeat_interval: 15m
      continue: true

    # Service-specific routes
    - match:
        component: service
      receiver: 'service-alerts'
      repeat_interval: 1h
      continue: true

    # Error monitoring routes
    - match:
        component: errors
      receiver: 'error-alerts'
      repeat_interval: 30m
      continue: true

# Alert receivers (notification destinations)
receivers:
  # Default receiver - WebSocket broadcast to frontend
  - name: 'default'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true
        http_config:
          follow_redirects: true
        max_alerts: 0  # No limit on alerts per request

  # Critical alerts - WebSocket + optional email/Slack
  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true
    # Uncomment when email is configured:
    # email_configs:
    #   - to: '${ALERT_EMAIL_TO}'
    #     send_resolved: true
    #     headers:
    #       Subject: 'ðŸ”¥ CRITICAL: {{ .GroupLabels.alertname }}'
    # Uncomment when Slack is configured:
    # slack_configs:
    #   - channel: '#autobot-alerts'
    #     send_resolved: true
    #     title: 'ðŸ”¥ CRITICAL: {{ .GroupLabels.alertname }}'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'

  # High severity alerts
  - name: 'high-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

  # Medium severity alerts
  - name: 'medium-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

  # Circuit breaker alerts (Issue #69)
  - name: 'circuit-breaker-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true
    # Uncomment for Slack notifications:
    # slack_configs:
    #   - channel: '#autobot-alerts'
    #     send_resolved: true
    #     title: 'âš¡ Circuit Breaker: {{ .GroupLabels.alertname }}'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.recommendation }}\n{{ end }}'

  # Redis alerts (Issue #69)
  - name: 'redis-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

  # NPU alerts (Issue #69)
  - name: 'npu-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

  # Security alerts (Issue #69) - high priority
  - name: 'security-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true
    # Uncomment for email notifications:
    # email_configs:
    #   - to: '${ALERT_SECURITY_EMAIL}'
    #     send_resolved: true
    #     headers:
    #       Subject: 'ðŸš¨ SECURITY: {{ .GroupLabels.alertname }}'

  # Service alerts
  - name: 'service-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

  # Error alerts
  - name: 'error-alerts'
    webhook_configs:
      - url: 'http://172.16.168.20:8001/api/webhook/alertmanager'
        send_resolved: true

# Inhibition rules - suppress alerts when other alerts are firing
inhibit_rules:
  # Don't alert on high CPU if critical CPU is firing
  - source_match:
      alertname: 'CriticalCPUUsage'
    target_match:
      alertname: 'HighCPUUsage'
    equal: ['component']

  # Don't alert on high memory if critical memory is firing
  - source_match:
      alertname: 'CriticalMemoryUsage'
    target_match:
      alertname: 'HighMemoryUsage'
    equal: ['component']

  # Don't alert on high disk if critical disk is firing
  - source_match:
      alertname: 'CriticalDiskUsage'
    target_match:
      alertname: 'HighDiskUsage'
    equal: ['component']

  # Don't alert on service performance if service is down
  - source_match:
      severity: 'critical'
      component: 'service'
    target_match:
      severity: 'medium'
      component: 'service'
    equal: ['service']

  # Circuit Breaker Inhibitions (Issue #69)
  # Don't alert on high failures if circuit is already open
  - source_match:
      alertname: 'CircuitBreakerOpen'
    target_match:
      alertname: 'CircuitBreakerHighFailures'
    equal: ['database']

  # Don't alert on half-open if circuit is fully open
  - source_match:
      alertname: 'CircuitBreakerOpen'
    target_match:
      alertname: 'CircuitBreakerHalfOpen'
    equal: ['database']

  # Don't alert on event spikes if circuit is already open
  - source_match:
      alertname: 'CircuitBreakerOpen'
    target_match:
      alertname: 'CircuitBreakerEventSpike'
    equal: ['database']

  # Redis Inhibitions (Issue #69)
  # Don't alert on latency/pool if Redis is completely down
  - source_match:
      alertname: 'RedisServerDown'
    target_match:
      component: 'redis'
    equal: ['database']

  # NPU Inhibitions (Issue #69)
  # Don't alert on high failure rate if circuit breaker is open
  - source_match:
      alertname: 'NPUCircuitBreakerOpen'
    target_match:
      alertname: 'NPUHighFailureRate'
    equal: ['component']

  # Resource Inhibitions (Issue #69)
  # Don't alert on connection exhaustion if Redis is down
  - source_match:
      alertname: 'RedisServerDown'
    target_match:
      alertname: 'ConnectionExhaustion'
    equal: ['database']
