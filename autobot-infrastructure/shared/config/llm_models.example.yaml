# LLM Model Configuration
# Defines context window capabilities and message budgeting per model

models:
  # Default fallback model
  default:
    name: "qwen2.5-coder-7b-instruct"
    context_window_tokens: 4096
    max_output_tokens: 2048
    message_budget:
      system_prompt: 500
      recent_messages: 20
      max_history_tokens: 3000

  # Qwen 2.5 Coder 7B
  qwen2.5-coder-7b-instruct:
    context_window_tokens: 4096
    max_output_tokens: 2048
    message_budget:
      system_prompt: 500
      recent_messages: 20
      max_history_tokens: 3000

  # Qwen 2.5 Coder 14B
  qwen2.5-coder-14b-instruct:
    context_window_tokens: 8192
    max_output_tokens: 4096
    message_budget:
      system_prompt: 800
      recent_messages: 30
      max_history_tokens: 6000

  # Llama 3.2 3B
  llama-3.2-3b-instruct:
    context_window_tokens: 4096
    max_output_tokens: 2048
    message_budget:
      system_prompt: 400
      recent_messages: 15
      max_history_tokens: 2800

  # Phi-3 Mini
  phi-3-mini-4k-instruct:
    context_window_tokens: 4096
    max_output_tokens: 2048
    message_budget:
      system_prompt: 300
      recent_messages: 15
      max_history_tokens: 3000

# Token estimation defaults
token_estimation:
  chars_per_token: 4  # Conservative estimate
  safety_margin: 0.9  # Use 90% of available context

# Model Performance Classification
# Maps models to performance levels for intelligent task routing
performance_classification:
  # Lightweight models (< 2B parameters) - Fast, low resource usage
  lightweight:
    - llama3.2:1b
    - gemma3:270m
    - gemma3:1b
    - nomic-embed-text

  # Standard models (2-8B parameters) - Balanced performance/quality
  standard:
    - llama3.2:3b
    - gemma2:2b
    - gemma3:latest
    - artifish/llama3.2-uncensored
    - qwen2.5-coder-7b-instruct

  # Advanced models (8B+ parameters) - High quality, more resources
  advanced:
    - dolphin-llama3:8b
    - wizard-vicuna-uncensored:13b
    - qwen2.5-coder-14b-instruct
